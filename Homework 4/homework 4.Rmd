---
title: "Homework 4"
author: "Frank Chou, Tejaswi Pukkalla"
date: "April 23, 2019"
output: pdf_document
fontsize: 12
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Question 1: Clustering and PCA
Given the fact that the **wines.csv** dataset consists of 11 chemical properties and 6,500 different bottles of *vinho verde* wine from northern Portugal, we have some difficulty determining which dimensionality reduction technique would be the better model for the task. With only 11 variables of the chemical properties of the wine, **Principal Component Analysis (PCA)** offers a easy-to-use tool to reduce the total number of variables down to a handful to work with, however at the same time, important information might be lost if the data is reduced to just a handful. On the other hand, **clustering** offers a method where all of the variables are at play, meaning that all 11 variables would be considered when determining which wine would be a part of which artificial group we are creating. However the drawback is that given the inherent randomization of cluster selection and our algorithm settings, we would have minute yet inconsistent results.

## Principal Component Analysis
```{r pca initialization, include=FALSE}
ip <- installed.packages() 
pkgs.to.remove <- ip[!(ip[,"Priority"] %in% c("base", "recommended")), 1] 

library(ISLR)
library(tidyverse)
library(ggplot2)
library(psych)
library(xtable)

wine <- read.csv("wine.csv", header = TRUE)

# Pick out the pca columns
Z = wine[,c(1:9)]
# Standardize (center/scale) the data
Z_std = scale(Z, center=TRUE, scale=TRUE)
plot(Z_std)

x_wine = as.matrix(wine[,1:11])
y_wine_quality= wine[,12]
y_wine_color= wine[,13]

# Let's try dimensionality reduction via PCA
pc_wine = prcomp(x_wine, scale=TRUE)

# pc_wine$x has the summary variables
# Regress on the first K
K = 3
scores = pc_wine$x[,1:K]
pcr1 = lm(y_wine_quality ~ scores)


```
Our first dimensionality reduction technique would be Principal Component Analysis or PCA. In this case, the goal is to find low-dimensional summaries of high-dimensional data sets. As mentioned before, given the fact that our dataset only has 11 feature variables, the determination of what is high and low is a factor here. 11 variables by itself, is a rather low number of variables in play. However given its mathematical foundations, utilizing linear algebra to determine a vector subspace of the overall data, we capture the entirety of the data when determining our principal components.

```{r summary of pca, results = 'asis',echo=FALSE}
print(xtable(summary(pcr1)),comment=FALSE)

```
With the results of our PCA regression, we find that with just three PCA components, we are able to find a model that has **residual standard error** `r summary(pcr1)[[6]]` that captures a significant portion of the observations within the data. In addition, each coefficient's p-values indicate that each are well within the confidence level needed to utilize each principal component accurately.

\pagebreak

```{r matrix plot,echo=FALSE}
# fancy plot matrix with stuff, see http://www.sthda.com/english/wiki/scatter-plot-matrices-r-base-graphs
pairs.panels(Z_std[,1:6], 
             method = "pearson", # correlation method
             hist.col = "#00AFBB",
             density = TRUE,  # show density plots
             ellipses = TRUE # show correlation ellipses
)
```
In this matrix of different plots, we take 6 from the 11 descriptor variables in the dataset and display the following:

* Bi-variate scatter plots below the diagonal
* Histograms on the diagonal
* Pearson correlation above the diagonal.

In the bi-variate scatter plots, we find a visual correlation between different variables. By utilizing PCA we can safely presume that linear combinations of similar variables would be a suitable approach in decomposing the data to a handful of variables.

While in the histograms on the diagonal, we see that once we normalize the underlying dataset, we would have a large concentration of values in the left hand side of the total range. Further supporting the notion that there is high correlation of similar wines.

Lastly, in the Pearson correlation, the rather low (relative to zero) values hints that there is little support for multicollinearity among the variables. Ultimately, PCA is one of two approaches we used to understand the data.

\pagebreak

```{r PCA Scatterplot,echo=FALSE}
# Show the model fit
par(mfrow=c(2,2))
plot(fitted(pcr1), y_wine_quality, main="Wine Quality and PC 1", ylab = "Wine Quality",xlab = "Estimated Wine Quality from PCA")
plot(seq(1,11,by=1), pc_wine$rotation[,1], main="Coefficients of PC 1", ylab = "PC 1 Effects",xlab = "PC Input Variables")
plot(seq(1,11,by=1), pc_wine$rotation[,2], main="Coefficients of PC 2", ylab = "PC 2 Effects",xlab = "PC Input Variables")
plot(seq(1,11,by=1), pc_wine$rotation[,3], main="Coefficients of PC 3", ylab = "PC 3 Effects",xlab = "PC Input Variables")
```
Before we move to our other approach, clustering, we present a 2x2 graph depicting the relationship of each PCA component among another. Given the scattered distribution of points, we find that there is considerable difficulty discerning the quality of wine based on the betas derived from each principal component.

\pagebreak

## Clustering
```{r clustering initilization,include=FALSE}
library(ggplot2)
library(LICORS)  # for kmeans++
library(foreach)
library(mosaic)
library(gridExtra)
library(grid)

# Center and scale the data
X = wine[,c(1:9)]
X = scale(X, center=TRUE, scale=TRUE)

# Extract the centers and scales from the rescaled data (which are named attributes)
mu = attr(X,"scaled:center")
sigma = attr(X,"scaled:scale")

# Run k-means with 2 clusters and 25 starts
clust1 = kmeans(X, 2, nstart=25)

# Using kmeans++ initialization
clust2 = kmeanspp(X, k=2, nstart=25)
```

Our second method would be to apply a clustering algorithm on the data. Here we will apply a supervised learning algorithm to determine which wine type and quality level each of the wines in our dataset would be. Given our rather small categorical classifications: **red** or **white** this approach would be accurate enough to correctly classify the majority of the wines in question. Given our linear scale, be able to predict the wine quality within a close parameter of the underlying wine judged quality level. With only two clusters to predict, we have an easier time implementing the code to achieve better results.

```{r What are the clusters?, echo=FALSE}
# What are the clusters?
print("Cluster 1")
clust2$center[1,]*sigma + mu
print("Cluster 2")
clust2$center[2,]*sigma + mu
```
Here we have the criteria that the algorithm devised to determine which wine would be assigned to which cluster category. We see that given two clusters to work with, either one presumably to determine whether or not it is white or red wine, we can see distinct differences in all of the cutoff levels for each variable. 

\pagebreak

```{r Volatile and Fixed Acidity by Wine Type, echo=FALSE}
qplot(fixed.acidity, volatile.acidity, data=wine, color=factor(clust2$cluster),main = "Volatile and Fixed Acidity by Wine Type")+
  labs(colour = "Wine Type")
```
By comparing our two variable relating to acidity **Volatile Acidity** and **Fixed Acidity** we find that the clustering algorithm has captured the two main groups of wine with an intermixed amount near the middle  diagonal. As we depict more comparisons, we will find that the cluster approach produces an easier to understand graph of the results.


\pagebreak

```{r Citric Acid and Residual Sugar by Wine Type by Wine Type, echo=FALSE}
qplot(citric.acid, residual.sugar, data=wine, color=factor(clust2$cluster),main = "Citric Acid and Residual Sugar by Wine Type by Wine Type")+
  labs(colour = "Wine Type")
```
Here is an example where clustering failed to adequately determine the correct wine type based on the **Residual Sugar** and **Citric Acid** samples of the wine. With **Red** as *1* and **White** wines tagged as *2*, we find that the clustering method incorrectly created a large centroid of wines near the center of the mass.

\pagebreak

```{r Wine Qualiy and Color by actual Wine Type, echo=FALSE}
qplot(quality, color, data=wine, color=factor(clust2$cluster),main = "Wine Qualiy and Color by actual Wine Type")+
  labs(colour = "Wine Type")
```
Lastly we have the final clustering of the wine **Color** and **Quality**. The clustering approach correctly labels all but one of the white wines while missing all but two of the reds. Given the fact that there are only colors of wines to work with, missing even one or two of the wine set would mean that although a cluster approach would be easier to implement, it will still miss 10% or more of the time when determining whether or not a wine is red or white. 

\pagebreak

# Question 2: Market Segmentation