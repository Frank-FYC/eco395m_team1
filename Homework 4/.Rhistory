install.packages("LICORS")
source('C:/Dropbox/Classes/ECO 395M Data mining and statisical learning/2019-04-10 class/cars.R', echo=TRUE)
source('C:/Dropbox/Classes/ECO 395M Data mining and statisical learning/2019-04-10 class/cars.R', echo=TRUE)
source('C:/Dropbox/Classes/ECO 395M Data mining and statisical learning/2019-04-10 class/linkage_minmax.R', echo=TRUE)
# Run hierarchical clustering with single (min) linkage
# here min produces counterintuitive results
x_dist = dist(x)
h1 = hclust(x_dist, method='single')
c1 = cutree(h1, 2)
# Run hierarchical clustering with single (min) linkage
# here min produces counterintuitive results
x_dist = dist(x)
h1 = hclust(x_dist, method='single')
c1 = cutree(h1, 2)
D = data.frame(x, z = c1)
ggplot(D) + geom_point(aes(x=X1, y=X2, col=factor(z)))
# Run hierarchical clustering with average linkage
# Run hierarchical clustering with average linkage
h3 = hclust(x_dist, method='average')
c3 = cutree(h3, 2)
D3 = data.frame(x, z = c3)
ggplot(D3) + geom_point(aes(x=X1, y=X2, col=factor(z)))
# Run hierarchical clustering with average linkage
h3 = hclust(x_dist, method='average')
c3 = cutree(h3, 2)
D3 = data.frame(x, z = c3)
ggplot(D3) + geom_point(aes(x=X1, y=X2, col=factor(z)))
# But here's a different example where max produces counterintuitive results
set.seed(84958)
mu1 = c(-1, 0)
mu2 = c(1, 0)
sigma1 = diag(0.1^2, 2)
sigma2 = diag(0.45^2, 2)
x1 = rmvnorm(250, mu1, sigma1)
x2 = rmvnorm(250, mu2, sigma2)
x = rbind(x1, x2)
plot(x)
# Run hierarchical clustering with single (min) linkage
x_dist = dist(x)
h1 = hclust(x_dist, method='single')
c1 = cutree(h1, 2)
D = data.frame(x, z = c1)
ggplot(D) + geom_point(aes(x=X1, y=X2, col=factor(z)))
# Run hierarchical clustering with complete (max) linkage
h2 = hclust(x_dist, method='complete')
c2 = cutree(h2, 2)
D2 = data.frame(x, z = c2)
ggplot(D2) + geom_point(aes(x=X1, y=X2, col=factor(z)))
# Run hierarchical clustering with average linkage
h3 = hclust(x_dist, method='average')
c3 = cutree(h3, 2)
D3 = data.frame(x, z = c3)
ggplot(D3) + geom_point(aes(x=X1, y=X2, col=factor(z)))
source('C:/Dropbox/Classes/ECO 395M Data mining and statisical learning/eco395m_team_awesome/Homework 4/gasoline.R', echo=TRUE)
setwd("C:/Dropbox/Classes/ECO 395M Data mining and statisical learning/eco395m_team_awesome/Homework 4")
source('C:/Dropbox/Classes/ECO 395M Data mining and statisical learning/eco395m_team_awesome/Homework 4/gasoline.R', echo=TRUE)
wine <- read.csv("C:/Dropbox/Classes/ECO 395M Data mining and statisical learning/eco395m_team_awesome/Homework 4/wine.csv")
View(wine)
wine <- read.csv("wine.csv")
View(wine)
unique(wines$color)
unique(wine$color)
wine <- read.csv("wine.csv", header = TRUE)
X = as.matrix(wine[,-1])
y = wine[,1]
source('C:/Dropbox/Classes/ECO 395M Data mining and statisical learning/eco395m_team_awesome/Homework 4/gasoline.R', echo=TRUE)
source('C:/Dropbox/Classes/ECO 395M Data mining and statisical learning/eco395m_team_awesome/Homework 4/gasoline.R', echo=TRUE)
source('C:/Dropbox/Classes/ECO 395M Data mining and statisical learning/eco395m_team_awesome/Homework 4/gasoline.R', echo=TRUE)
View(gasoline)
names(gasoline)
source('C:/Dropbox/Classes/ECO 395M Data mining and statisical learning/eco395m_team_awesome/Homework 4/gasoline.R', echo=TRUE)
scores = pc_gasoline$x_gasoline[,1:K]
pcr1 = lm(y_gasoline ~ scores)
pc_gasoline$x_gasoline[,1:K]
View(pc_gasoline)
scores = pc_gasoline$x[,1:K]
source('C:/Dropbox/Classes/ECO 395M Data mining and statisical learning/eco395m_team_awesome/Homework 4/gasoline.R', echo=TRUE)
wine <- read.csv("wine.csv", header = TRUE)
x_wine = as.matrix(wine[,-1])
y_wine = wine[,1]
# Let's try dimensionality reduction via PCA
pc_wine = prcomp(x_wine, scale=TRUE)
# pc_gasoline$x_wine has the summary variables
# Regress on the first K
K = 3
scores = pc_gasoline$x[,1:K]
pcr1 = lm(y_wine ~ scores)
summary(pcr1)
# Show the model fit
plot(fitted(pcr1), y_wine)
wine <- read.csv("wine.csv", header = TRUE)
x_wine = as.matrix(wine[,-1])
y_wine = wine[,1]
# Let's try dimensionality reduction via PCA
pc_wine = prcomp(x_wine, scale=TRUE)
# pc_gasoline$x_wine has the summary variables
# Regress on the first K
K = 3
scores = pc_wine$x[,1:K]
pcr1 = lm(y_wine ~ scores)
summary(pcr1)
# Show the model fit
plot(fitted(pcr1), y_wine)
pcr1 = lm(y_wine ~ scores)
scores = pc_wine$x[,1:K]
# Let's try dimensionality reduction via PCA
pc_wine = prcomp(x_wine, scale=TRUE)
# Let's try dimensionality reduction via PCA
pc_wine = prcomp(x_wine, scale=TRUE)
x_wine = as.matrix(wine[,-1])
y_wine = wine[,1]
# Let's try dimensionality reduction via PCA
pc_wine = prcomp(x_wine, scale=TRUE)
x_wine
source('C:/Dropbox/Classes/ECO 395M Data mining and statisical learning/eco395m_team_awesome/Homework 4/gasoline.R', echo=TRUE)
x_gasoline
x_wine
names(x_gasoline)
x_gasoline
head(x_gasoline)
y_wine
x_wine = as.matrix(wine[,1])
x_wine
y_wine = wine[,-1]
View(x_wine)
View(y_wine)
ncol(wine)
x_wine = as.matrix(wine[,1:11])
y_wine = wine[,12:13]
View(x_wine)
View(y_wine)
y_wine_quality= wine[,12]
y_wine_color= wine[,13]
# Let's try dimensionality reduction via PCA
pc_wine = prcomp(x_wine, scale=TRUE)
# pc_gasoline$x_wine has the summary variables
# Regress on the first K
K = 3
scores = pc_wine$x[,1:K]
pcr1 = lm(y_wine_quality ~ scores)
summary(pcr1)
# Show the model fit
plot(fitted(pcr1), y_wine)
# Show the model fit
plot(fitted(pcr1), y_wine_quality)
nir_wavelength
# Visualize the first few principal components:
# these are the coefficients in the linear combination for each summary
plot(x_wine, pc_gasoline$rotation[,1])
pc_gasoline$rotation[,1]
pc_wine$rotation[,1]
# Visualize the first few principal components:
# these are the coefficients in the linear combination for each summary
plot(x_wine, pc_wine$rotation[,1])
length(pc_wine)
pc_wine
as.data.frame(pc_wine)
nir_wavelength
# Visualize the first few principal components:
# these are the coefficients in the linear combination for each summary
plot(nir_wavelength, pc_gasoline$rotation[,1], ylim=c(-0.15,0.15))
pc_gasoline$rotation[,1]
nrows(wine)
nrow(wine)
pc_wine$rotation[,1]
View(pc_wine)
pc_wine[["rotation"]]
length(pc_wine$rotation[,1])
length(x_wine)
nrow(x_wine)
length(x_wine)
nrow(pc_gasoline$rotation[,1])
length(pc_gasoline$rotation[,1])
# Visualize the first few principal components:
# these are the coefficients in the linear combination for each summary
plot(x_wine, pc_wine$rotation[,1])
length(nir_wavelength)
length(pc_gasoline$rotation[,1])
length(pc_wine$rotation[,1])
length(x_wine)
length(pc_wine)
nrow(pc_wine)
length(x_wine)
nrow(x_wine)
mu_x
y_wine_quality
fitted(pcr1)
# Show the model fit
plot(fitted(pcr1), y_wine_quality)
# Show the model fit
plot(fitted(pcr1), y_wine_quality)
summary(pcr1)
K = 4
scores = pc_wine$x[,1:K]
pcr1 = lm(y_wine_quality ~ scores)
summary(pcr1)
K = 11
scores = pc_wine$x[,1:K]
pcr1 = lm(y_wine_quality ~ scores)
summary(pcr1)
K = 3
scores = pc_wine$x[,1:K]
pcr1 = lm(y_wine_quality ~ scores)
summary(pcr1)
length(pcr1)
length(pc_wine$rotation[,1])
# Visualize the first few principal components:
# these are the coefficients in the linear combination for each summary
plot(pcr1, pc_wine$rotation[,1])
pc_gasoline$rotation[,1]
length(fitted(pcr1))
length(pcr1)
pcr1
# Visualize the first few principal components:
# these are the coefficients in the linear combination for each summary
plot(seq(1-11,by=1), pc_wine$rotation[,1])
seq(1-11,by=1)
# Visualize the first few principal components:
# these are the coefficients in the linear combination for each summary
plot(seq(1,11,by=1), pc_wine$rotation[,1])
# Visualize the first few principal components:
# these are the coefficients in the linear combination for each summary
plot(seq(1,11,by=1), pc_wine$rotation[,1])
plot(fitted(pcr1), y_wine_quality)
# we can find best k, through train test splits, data validation. Auto-encoder = PCA. Dimensionality reduction.
# Visualize the first few principal components:
# these are the coefficients in the linear combination for each summary
plot(seq(1,11,by=1), pc_wine$rotation[,1])
plot(seq(1,11,by=1), pc_wine$rotation[,2])
plot(seq(1,11,by=1), pc_wine$rotation[,3])
# Show the model fit
plot(fitted(pcr1), y_wine_quality)
# Visualize the first few principal components:
# these are the coefficients in the linear combination for each summary
plot(seq(1,11,by=1), pc_wine$rotation[,1])
plot(seq(1,11,by=1), pc_wine$rotation[,2])
plot(seq(1,11,by=1), pc_wine$rotation[,3])
