---
title: "Homework 2"
author: "Frank Chou, Milo Opdahl, Tejaswi Pukkalla"
date: "March 12, 2019"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Question 2: A Hospital Audit

```{r question 2 code,include=FALSE}
rm(list=ls())

library(MASS) 	## a library of example datasets
library(tidyverse)
library(nnet)  # for multinom
library(mosaic)

options(warn=-1)

brca <- read.csv("./brca.csv")

#####
#First question: are some radiologists more clinically conservative than others in recalling patients, holding patient risk factors equal?

# Second question: when the radiologists at this hospital interpret a mammogram to make a decision on whether to recall the patient, does the data suggest that they should be weighing some clinical risk factors more heavily than they currently are?

rmse = function(y, yhat) {
  sqrt( mean( (y - yhat)^2 ) )
}

df_rad13 = subset(brca,radiologist=="radiologist13")
df_rad34 = subset(brca,radiologist=="radiologist34")
df_rad66 = subset(brca,radiologist=="radiologist66")
df_rad89 = subset(brca,radiologist=="radiologist89")
df_rad95 = subset(brca,radiologist=="radiologist95")

probtest = function(a,b){
  n = nrow(a)
  n_train = round(0.8*n) #take x times n, then round to nearest
  n_test = n - n_train #number of n test is n minus n_train
  
  train_ind = sort(sample.int(n, n_train, replace=FALSE)) # randomly sample from n, n_train times, without replacement, then sort
  brca_train = select(a[train_ind,],-radiologist)
  brca_test = select(a[-train_ind,],-radiologist)
  brca.w_test = select(subset(brca,radiologist != b),-radiologist)
  
  # train models: recall
  maxit <<- 10000
  
  lm1 <<- glm(recall ~ .-cancer,
            data=brca_train,
            maxit = maxit)
  lm2 <<- glm(recall ~ (.-cancer)^2, 
            data=brca_train, 
            maxit = maxit)
  # train models: cancer
  lm3 <<- glm(cancer ~ recall,
              data=brca_train,
              maxit = maxit)
  lm4 <<- glm(cancer ~ .,
              data=brca_train,
              maxit = maxit)
  lm5 <<- glm(cancer ~ .-recall, 
              data=brca_train, 
              maxit = maxit)
  lm6 <<- glm(cancer ~ (.-recall)^2, 
              data=brca_train, 
              maxit = maxit)
  
  # predict on the specific radiologist testing set
  yhat_test1 = predict(lm1,brca_test)
  yhat_test2 = predict(lm2,brca_test) 
  
  yhat_test1.w = predict(lm1,brca.w_test)
  yhat_test2.w = predict(lm2,brca.w_test)
  
  yhat_test3 = predict(lm3,brca_test)
  yhat_test4 = predict(lm4,brca_test)
  yhat_test5 = predict(lm5,brca_test)
  yhat_test6 = predict(lm6,brca_test)
  
  yhat_test3.w = predict(lm3,brca.w_test)
  yhat_test4.w = predict(lm4,brca.w_test)
  yhat_test5.w = predict(lm5,brca.w_test)
  yhat_test6.w = predict(lm6,brca.w_test)
  
  # predict on the other radiologists testing set
  
  rmse = function(y, yhat) {
    sqrt( mean( (y - yhat)^2 ) )
  }
  
  c(rmse(brca_test$recall, yhat_test1),
    rmse(brca_test$recall, yhat_test2),
    
    rmse(brca.w_test$recall, yhat_test1.w),
    rmse(brca.w_test$recall, yhat_test2.w),
    
    rmse(brca_test$recall, yhat_test3),
    rmse(brca_test$recall, yhat_test4),
    rmse(brca_test$recall, yhat_test5),
    rmse(brca_test$recall, yhat_test6),
    
    rmse(brca.w_test$recall, yhat_test3.w),
    rmse(brca.w_test$recall, yhat_test4.w),
    rmse(brca.w_test$recall, yhat_test5.w),
    rmse(brca.w_test$recall, yhat_test6.w)
  )
}

# radiologist13 radiologist34 radiologist66 radiologist89 radiologist95

rad13 = do(100)*{probtest(df_rad13,"radiologist13")}
rad34 = do(100)*{probtest(df_rad34,"radiologist34")}
rad66 = do(100)*{probtest(df_rad66,"radiologist66")}
rad89 = do(100)*{
  tryCatch({
  probtest(df_rad89,"radiologist89")
  }, error=function(e){cat("ERROR :",conditionMessage(e), "\n")})
}
rad95 = do(100)*{probtest(df_rad95,"radiologist95")}
superrad = do(100)*{probtest(brca,"")}

df = as.data.frame(
  rbind(
  colMeans(rad13),
  colMeans(rad34),
  colMeans(rad66),
  colMeans(rad89),
  colMeans(rad95),
  colMeans(superrad)
  )
)
rownames(df) = c("radiologist13","radiologist34","radiologist66","radiologist89","radiologist95","SuperRad")
colnames(df) = c("lm1","lm2","lm1.w","lm2.w","lm3","lm4","lm5","lm6","lm3.w","lm4.w","lm5.w","lm6.w")

df = as.data.frame(t(df))
df$Rad13.compare = (df$radiologist13 - df$SuperRad)
df$Rad34.compare = (df$radiologist34 - df$SuperRad)
df$Rad66.compare = (df$radiologist66 - df$SuperRad)
df$Rad89.compare = (df$radiologist89 - df$SuperRad)
df$Rad95.compare = (df$radiologist95 - df$SuperRad)
df = as.data.frame(t(df))

df2 = df[,c(1,3,2,4,5,9,6,10,7,11,8,12)]
df2 = as.data.frame(t(df2))


# a confusion table to determine the best way to detect cancer

n = nrow(brca)
n_train = round(0.8*n) #take x times n, then round to nearest
n_test = n - n_train #number of n test is n minus n_train

train_ind = sort(sample.int(n, n_train, replace=FALSE)) # randomly sample from n, n_train times, without replacement, then sort
brca_train = brca[train_ind,]
brca_test = brca[-train_ind,]

lm3 <<- glm(cancer ~ recall,
            data=brca_train,
            maxit = maxit)
lm4 <<- glm(cancer ~ .,
            data=brca_train,
            maxit = maxit)
lm5 <<- glm(cancer ~ .-recall, 
            data=brca_train, 
            maxit = maxit)
lm6 <<- glm(cancer ~ (.-recall)^2, 
            data=brca_train, 
            maxit = maxit)

confusion.table = function(x){
  probhat_test = predict(x, newdata=brca_test)
  yhat_test = ifelse(probhat_test >= 0.1, 1, 0)
  table(y=brca_test$cancer, yhat=yhat_test)
}

confusion.table(lm3)
confusion.table(lm4)
confusion.table(lm5)
confusion.table(lm6)

```

Hospital Audits are important to determine the effectiveness of hospital operations from a objective standpoint. In this particular case, the goal is to determining the performance of radiologists using a statistical audit of their recent patient interactions - a crucial link between modern data-science and hospital operations. Two overall questions are posited:

1. First question: are some radiologists more clinically conservative than others in recalling patients, holding patient risk factors equal?

2. Second question: when the radiologists at this hospital interpret a mammogram to make a decision on whether to recall the patient, does the data suggest that they should be weighing some clinical risk factors more heavily than they currently are?

At the core of each question is reducing the number of false negatives - where a radiologist recommends a patient to conduct further tests and thereby allows a patient to begin immediately; and false positives - where a radiologist recommends further tests but ultimately turns out that there was no cancer. By introducing a statistical model, the goal is to augment the predictive capabilities of radiologist and offer a better standard of care for patients.

This audit is structured in four parts: first is a brief summary of the data and how it is structured, second is a demonstration and presentation of answering question one, third is a similar approach for question two, fourth is a conclusion of the audit's findings and recommendations for improvement of future radiologist performance or audit effectiveness.

## Part One: Brief Summary of Data
```{r summary statistics, echo=FALSE}
summary(brca)
```
The data of mammograms used in this audit were selected from a Hospital in Seattle, Washington. At this hospital, five radiologists were selected at random for the audit - where about 200 mammograms were randomly selected from the hospital for each. For a total of 987 mammograms covering 7 parameters:

* age: 40-49*, 50-59, 60-69, 70 and older

* family history of breast cancer: 0=No*, 1=Yes

* history of breast biopsy/surgery: 0=No*, 1=Yes

* breast cancer symptoms: 0=No*, 1=Yes

* menopause/hormone-therapy status: Pre-menopausal, Post-menopausal & no hormone replacement therapy (HT), Post-menopausal & HT*, Post-menopausal & unknown HT

* previous mammogram: 0=No*, 1=Yes

* breast density classification: 1=Almost entirely fatty, 2=Scattered fibroglandular tissue*, 3=Heterogeneously dense, 4=Extremely dense

Of these factors, two are of special interest: [recall] and [cancer]. In the abstract [recall] can be explained as the following: upon seeing the medical history of a patient, they can either recommend either one of two actions: recall for further screening or not. It is presumed that radiologists utilize all of the information available before they make a decision. This implies that there is a inherent correlative factor between recall and patient history. On the other hand [cancer] is whether or not a patient, whether through the recall screening process, or through another pathway of discovery - develops cancer within a 12 month window after seeing the radiologist. 

## Part Two: Clinical Conservativism
Without knowing how patients are assigned to radiologists, it is presumed that the relationship is random at best, and preferential at worst. With a random assignment, we can presume that each radiologist chosen for the audit would have seen, on average, the same makeup of patients that would necessitate a mammogram. A random assignment would entail a random drawing of cancer patients from the overall total cancer patient pool from the population. If preferential - meaning that a patient approaches a radiologist and requests care and upon the approval of the radiologist, we see an issue of sampling error within the audit data; as there is a bias introduced between patient selection and radiologist. Radiologist may either self-select for more difficult cases or easier based on preference and patients self-select based on their estimate of the reputation of the radiologist within the medical community. 

Regardless of assignment, the primary method of which we rank the clinical conservationism is to create a model that is trained on each of the radiologists' and then test the model on data from both the radiologist and other patients not seen by the radiologist in question. The goals behind this approach are twofold: one is to recreate a evaluation profile of the radiologist through a linear model of determining whether or not a patient should be recalled, two to determine whether or not a patient who is recalled or not develops cancer within a 12 month time frame.

The table below depicts the Root Mean Squared Error (RMSE) of each radiologist's model tested on a small sub-sample of the radiologist's test data and other radiologists' testing data.
```{r table of results,echo=FALSE}
t(df2)[,c(1,2,3,4)]
```
Example: 

* **radiologist13**: we have a the same linear model, **lm1 = glm(recall ~ .-cancer, data=brca_train, maxit = maxit),** trained to 20% of radiologist13's sample data as well as the whole mammogram data - excluding radiologist13's.

* **SuperRad**: is a model trained on a 20% random sample of the whole data set and tested on the remainder of the whole data set. This pseudo-radiologist serves as the benchmark for comparing radiologists to an artificial standard if one radiologist had access and saw all of the patients from the data set.

* **Rad13.compare**: is determined by subtracting the model RMSE result of ***radiologist13** by **SuperRad**. A positive value means that a model trained on **radiologist13's** training data did worse once it was tested on out of sample testing data and vice-versa.

```{r lm1 results, echo=FALSE}
t(df2)[,c(1,2,3,4)]
```

Just by viewing the table, it can be clearly discerned under **lm1** that on average, radiologists 13, 66, and 89 had worse performance than the benchmark **SuperRad** when looking at the RadXX.compare values for each radiologist; while 34 and 95 had better performance. But when we examine the results of each radiologists' model tested on the global data set, we find that on average, all radiologists were worse off. However **lm1** is a linear regression involving non-interacting variables from the data set. If we were to examine **lm2 <<- glm(recall ~ (.-cancer)^2,data=brca_train,maxit = maxit)** where we interact every variable with itself and another we find different results. Radiologist 95's model performance flips and becomes worse with 95's within-sample data. But once tested on the global data set, all radiologists' models performed worse than the benchmark. The takeaway from this analysis demonstrates that human radiologists, on average, are not as effective in determining whether or not a patient should be recalled than a statistical model. Although this might increase the number of false positives and false negatives, the overall increase in cancer detection would allow immediate treatment for true positives who otherwise would have gone undiagnosed. As for whether or not this behavior can be determined to be clinically conservative, meaning that radiologist will opt to recall a patient even if the clinical factors do not signal a need to recall, the distinction is minimal at best and hard to determine as all of the radiologists selected in the audit perform marginally better or worse than the benchmark.

## Part Three: Weighing Different Clinical Risk Factors

We first approach this question by developing four linear models that attempts to predict cancer rates based on the parameters available in the data set.

* lm3 <<- glm(cancer ~ recall,data=brca_train,maxit = maxit)
* lm4 <<- glm(cancer ~ recall + history,data=brca_train,maxit = maxit)
* lm5 <<- glm(cancer ~ .,data=brca_train,maxit = maxit)
* lm6 <<- glm(cancer ~ (.)^2,data=brca_train,maxit = maxit)

Because the goal of this question is to determine whether or not radiologists are effectively utilizing all of a patient's clinical data to determine whether or not to recall a patient, we first examine **lm3** and **lm4**. Both are linear models designed to find the partial effect of whether or not a patient was recalled and if they developed cancer within the next 12 months. However the distinction is that **lm3** only has recall as its x variable while **lm4** has both recall and family history.

```{r summary of lm3 and lm5,echo=TRUE}
summary(lm3)

summary(lm5)
```

By itself, we can see that the **recall** variable has a very significant (p-value close to 0) and large effect on whether or not a patient develops cancer. This makes sense because upon evaluating a patient, a radiologist will then determine whether or not the patient will be recalled and receive additional testing. Based on their experience and education, they will want to find the factors that most likely contributes to cancer. At the same time however, we also see significant (in terms of p-value and magnitude) effects from **age**, **menopause/hormone-therapy status**, and **breast density classification**. In light of these factors, a series of model efficacy tests were conducted to determine the effectiveness of different models.

```{r model tests, echo=FALSE}
t(df2)[,c(5,6,7,8,9,10,11,12)]
```

Looking across **SuperRad** we see that the RMSE of each model remains fairly consistent throughout the different implementation and test of each model - except when we exclude **recall** in models **lm5** and **lm6** . The exclusion of **recall** has a meaningful impact models' ability to guess the cancer rate for each patient. Given this puzzling outcome, the next step would be to examine **lm5.w** and **lm6.w** where we take models that exclude **recall** - after all, as recall determinations occur after a radiologist sees a patient and not before, we cannot use it to predict cancer; and see which radiologist model performs the best. Iteration terms seems to be resulting in higher RMSE in the predictive model than by itself. Given the summary results from earlier regarding the significance of some variables over others, it can be concluded that radiologists weigh **age**, **menopause/hormone-therapy status**, and **breast density classification** as indicators of cancer than other factors excluding recall.

## Part Four: Conclusion
Ultimately, it can be determined that human radiologists may appear to be more conservative than a statistical model, but the underlying analysis claims otherwise - the difference is small in nature and not of sufficient significance to sacrifice patient care for a more effective diagnosing mechanism. The number of false positives and false negatives remain small in comparison when the model changes from one to another. 

```{r confusion tables,echo=FALSE}
c.table_lm3 = confusion.table(lm3)
c.table_lm4 = confusion.table(lm4)
c.table_lm5 = confusion.table(lm5)
c.table_lm6 = suppressWarnings(confusion.table(lm6))

print("lm3 Confusion Table")
c.table_lm3
print("lm4 Confusion Table")
c.table_lm4
print("lm5 Confusion Table")
c.table_lm5
print("lm6 Confusion Table")
c.table_lm6
```

* Pair-wise guesses and actual cancer results. 
+ (0,0) means that a patient did not have caner and was not recalled.
+ (1,0) means that a patient had cancer but was not recalled.
+ (0,1) means that a patient did not have cancer but was recalled.
+ (1,1) means that a patient had cancer and was successfully recalled.

# Question 3: Going Viral

```{r question 3 code, include = FALSE}
rm(list=ls())

options(warn=-1)

library(mosaic)
library(tidyverse)
library(FNN)
library(foreach)
library(nnet)
require(scales)

df = read.csv('./online_news.csv')

df = subset(df, select = -c(url) )



rmse = function(y, yhat) {
  sqrt( mean( (y - yhat)^2 ) )
}

rmse_model = do(10)*{
  df = df[sample(nrow(df), 1000),]
  n = nrow(df)
  n_train = round(0.8*n) #take x times n, then round to nearest
  n_test = n - n_train #number of n test is n minus n_train
  
  train_ind = sort(sample.int(n, n_train, replace=FALSE)) # randomly sample from n, n_train times, without replacement, then sort
  df_train = df[train_ind,]
  df_test = df[-train_ind,]
  y_df_train = df_train$shares
  y_df_test = df_test$shares
  #####
  # train models: recall # num_videos data_channel_is_lifestyle 
  maxit <- 10000
  
  lm1 <<- glm(shares ~ .,
              data=df_train,
              maxit = maxit)
  lm2 <<- glm(shares ~ 
                n_tokens_content 
              + num_imgs 
              + data_channel_is_world 
              + max_negative_polarity,
              data=df_train,
              maxit = maxit)
  lm3 <<- glm(shares ~ 
              . 
              - n_tokens_content 
              - num_imgs 
              - data_channel_is_world
              - max_negative_polarity,
              data=df_train,
              maxit = maxit)
  lm4 <<- glm(shares ~ (.)^2,
              data=df_train,
              maxit = maxit)
  lm5 <<- glm(shares ~ 
                (n_tokens_content 
              + num_imgs 
              + data_channel_is_world 
              + max_negative_polarity)^2,
              data=df_train,
              maxit = maxit)
  lm6 <<- glm(shares ~ 
                (. 
              - n_tokens_content 
              - num_imgs 
              - data_channel_is_world
              - max_negative_polarity)^2,
              data=df_train,
              maxit = maxit)
  
  yhat_test1 = predict(lm1,df_test)
  yhat_test2 = predict(lm2,df_test)
  yhat_test3 = predict(lm3,df_test)
  yhat_test4 = predict(lm4,df_test)
  yhat_test5 = predict(lm5,df_test)
  yhat_test6 = predict(lm6,df_test)
  
  c(rmse(df_test$shares, yhat_test1),
    rmse(df_test$shares, yhat_test2),
    rmse(df_test$shares, yhat_test3),
    rmse(df_test$shares, yhat_test4),
    rmse(df_test$shares, yhat_test5),
    rmse(df_test$shares, yhat_test6)
  )
}

rmse_model
colMeans(rmse_model)

# in-sample and out of sample accuracy?
n = nrow(df)
n_train = round(0.8*n) #take x times n, then round to nearest
n_test = n - n_train #number of n test is n minus n_train

train_ind = sort(sample.int(n, n_train, replace=FALSE)) # randomly sample from n, n_train times, without replacement, then sort
df_train = df[train_ind,]
df_test = df[-train_ind,]

sample.accuracy = function(x){
  yhat_train = ifelse(predict(x) >= 1400, 1, 0)
  y_train = ifelse(df_train$shares >= 1400, 1, 0)
  in.sample_table <<- table(y=y_train, yhat=yhat_train)
  
  probhat_test = predict(x, newdata=df_test)
  yhat_test = ifelse(probhat_test >= 1400, 1, 0)
  y_test = ifelse(df_test$shares >= 1400, 1, 0)
  out.sample_table <<- table(y=y_test, yhat=yhat_test)
}

sample.accuracy(lm1)
in.sample_table
out.sample_table

sample.accuracy(lm2)
in.sample_table
out.sample_table

sample.accuracy(lm3)
in.sample_table
out.sample_table

sample.accuracy(lm4)
in.sample_table
out.sample_table

sample.accuracy(lm5)
in.sample_table
out.sample_table

sample.accuracy(lm6)
in.sample_table
out.sample_table

```

In the digital age, where information is no longer a constraint but rather - a superfluousness asset, determining what will be popular is a contentious task in of itself. Factors observable and unobservable go into the underlying decision-making of drawing a user's attention towards the consumption of given content. At the core of this question is determining what factors will ultimately predict the 'virality' given a piece of content and its associating metadata. To better understand this phenomena, a data set of 39,797 articles were utilized to train and test models to this effect. 

## Methodology

Given the large data set, it was computationally impractical to run the models on the entirety of the data set. A compromise was reached where 1000 articles were randomly sampled per cycle of model testing. Thereby maintaining independent and identically distributed random variables among the samples. Six different linear models were trained on 80% of this sampled data and tested on the remaining 20%. As mentioned before, a Root Mean Squared Error value was established among the models and then they were tested for in-sample and out-of-sample accuracy. As for deciding which factors played a role in determining whether or not content went viral, linear regression models were created and promising variables selected for further testing. 

The models used were the following

* lm1 <<- glm(shares ~ .,
            data=df_train,
            maxit = maxit)
* lm2 <<- glm(shares ~ 
              weekday_is_friday 
            + num_videos 
            + data_channel_is_lifestyle 
            + global_rate_negative_words,
            data=df_train,
            maxit = maxit)
* lm3 <<- glm(shares ~ 
            . 
            - weekday_is_friday 
            - num_videos 
            - data_channel_is_lifestyle
            - global_rate_negative_words,
            data=df_train,
            maxit = maxit)
* lm4 <<- glm(shares ~ (.)^2,
            data=df_train,
            maxit = maxit)
* lm5 <<- glm(shares ~ 
              (weekday_is_friday 
            + num_videos 
            + data_channel_is_lifestyle 
            + global_rate_negative_words)^2,
            data=df_train,
            maxit = maxit)
* lm6 <<- glm(shares ~ 
              (. 
            - weekday_is_friday 
            - num_videos 
            - data_channel_is_lifestyle
            - global_rate_negative_words)^2,
            data=df_train,
            maxit = maxit)

## Results

Because of computational limitation of the underlying base model, only sampling 1000 from a population of about 40,000, different iterations of training/testing cycles yields different results. As a result, only a general sense of what factors makes content go viral can be obtained at this time.

```{r p3 linear regression, echo=FALSE}
summary(lm1)
```
            
###The RMSE output for each model:

```{r question 3 rmse output, echo=FALSE}
rmse_result = as.data.frame(colMeans(rmse_model))
rownames(rmse_result) = c("lm1","lm2","lm3","lm4","lm5","lm6")
colnames(rmse_result) = c("RMSE")
rmse_result
```

### Confusion Matrixes of each Model on the sample 1000 set. 

```{r p3 confusion matrix, echo=FALSE}
print("lm1 Confusion Matrix, training and testing")
suppressWarnings(sample.accuracy(lm1))
in.sample_table
out.sample_table
print("lm2 Confusion Matrix, training and testing")
suppressWarnings(sample.accuracy(lm2))
in.sample_table
out.sample_table
print("lm3 Confusion Matrix, training and testing")
suppressWarnings(sample.accuracy(lm3))
in.sample_table
out.sample_table
print("lm4 Confusion Matrix, training and testing")
suppressWarnings(sample.accuracy(lm4))
in.sample_table
out.sample_table
print("lm5 Confusion Matrix, training and testing")
suppressWarnings(sample.accuracy(lm5))
in.sample_table
out.sample_table
print("lm6 Confusion Matrix, training and testing")
suppressWarnings(sample.accuracy(lm6))
in.sample_table
out.sample_table
```

## Conclusion
In hindsight, it is believed that the method of regress first, then threshold second, would be the optimal mechanism of developing models that predict what articles will go viral. The reasoning behind this presumption is that just as a model needs what factors that will make an article succeed, it will also need to know what factors causes it to not succeed. There are useful information and metrics from failure data that needs to be considered when construing predictive models.