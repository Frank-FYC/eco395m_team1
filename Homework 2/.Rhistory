library(foreach)
#####
# Example 1: inference for a binomial proportion
#####
# Goal: estimate P(cured | takes new drug)
# (as in a safety and efficacy trial, FDA phase 1)
# confidence level = 1-alpha
alpha = 0.05
# true probability
p_true = 0.25
# sample size
N = 100
# Let's simulate a single data set
results = rbinom(N, 1, p_true)
# step 1: calculate the estimator
p_hat = sum(results)/N
# step 2: plug in estimates of unknown quantities into
# analytical formula for standard error
se_hat = sqrt(p_hat * (1-p_hat)/N)
# step 3: form confidence interval based on asymptotic normality assumption
z_crit = qnorm(1 - alpha/2)
lower_bound = p_hat - z_crit*se_hat
upper_bound = p_hat + z_crit*se_hat
lower_bound
upper_bound
# check whether the interval contained the true value
(p_true > lower_bound) & (p_true < upper_bound)
# compare the normal CI with the bootstrapped CI
boot1 = do(1000)*{
results_bootstrap = resample(results)
p_hat_boot = sum(results_bootstrap)/N
}
# really close
confint(boot1)
# Let's repeat this in a Monte Carlo simulation
NMC = 1000
sim1 = do(NMC)*{
results = rbinom(N, 1, p_true)
p_hat = sum(results)/N
se_hat = sqrt(p_hat * (1-p_hat)/N)
lower_bound = p_hat - z_crit*se_hat
upper_bound = p_hat + z_crit*se_hat
(p_true > lower_bound) & (p_true < upper_bound)
}
sum(sim1$result)/NMC
# now repeat for many different values of N
n_grid = seq(6, 100, by=2)
clt_sim1 = foreach(N = n_grid, .combine='c') %do% { # %do% has an error
this_sim = do(NMC)*{
results = rbinom(N, 1, p_true)
p_hat = sum(results)/N
se_hat = sqrt(p_hat * (1-p_hat)/N)
lower_bound = p_hat - z_crit*se_hat
upper_bound = p_hat + z_crit*se_hat
(p_true > lower_bound) & (p_true < upper_bound)
}
sum(this_sim$result)/NMC
}
plot(n_grid, clt_sim1)
this_sim = do(NMC)*{
y = sample(population,N)
mu_hat = mean(y)
se_hat = sd(y)/sqrt(N)
lower_bound = mu_hat - z_crit*se_hat
upper_bound = muhat + z_crit*se_hat
(mu_true > lower_bound) & (mu_true < upper_bound)
}
sum(this_sim$result)/NMC
population
library(readr)
predimed <- read_csv("C:/Users/frank/Dropbox/Classes/ECO s394D Probability and Statistics [James Scott]/R Classes/2018-08-15 CI Simulation/predimed.csv")
xtabs(~event + group, data=predimed)
N = (97+1945)
p_phat = 97/N
p_phat
p_hat = 97/N
p_hat
N = (97+1945)
p_hat = 97/N
# step 2: std error
se_hat = sqrt(p_hat * (1-p_hat)/N)
se_hat
se_hat = sqrt(p_hat * (1-p_hat)/N)
z_crit = qnorm(1-0.05/2)
p_hat - z_crit*se_hat
p_hat + z_crit*se_hat
z_crit = qnorm((1-0.05)/2)
z_crit
z_crit = qnorm(1-0.05/2)
z_crit
citation()
demo()
quit()
update
help update
install.packages(c("backports", "BH", "broom", "cli", "colorspace", "digest", "dplyr", "evaluate", "fansi", "ggformula", "ggplot2", "jsonlite", "knitr", "markdown", "mime", "mosaic", "packrat", "pillar", "pkgconfig", "R6", "Rcpp", "readr", "RJSONIO", "rlang", "rmarkdown", "rsconnect", "rstudioapi", "stringi", "tibble", "tidyr", "tidyselect", "tinytex", "xfun"))
install.packages(c("backports", "BH", "broom", "cli", "colorspace", "digest", "dplyr", "evaluate", "fansi", "ggformula", "ggplot2", "jsonlite", "knitr", "markdown", "mime", "mosaic", "packrat", "pillar", "pkgconfig", "R6", "Rcpp", "readr", "RJSONIO", "rlang", "rmarkdown", "rsconnect", "rstudioapi", "stringi", "tibble", "tidyr", "tidyselect", "tinytex", "xfun"))
install.packages(c("backports", "BH", "broom", "cli", "colorspace", "digest", "dplyr", "evaluate", "fansi", "ggformula", "ggplot2", "jsonlite", "knitr", "markdown", "mime", "mosaic", "packrat", "pillar", "pkgconfig", "R6", "Rcpp", "readr", "RJSONIO", "rlang", "rmarkdown", "rsconnect", "rstudioapi", "stringi", "tibble", "tidyr", "tidyselect", "tinytex", "xfun"))
install.packages("rmarkdown")
# 1100: Why plots? So we can co-opt our visual processing to better understand numerical ratios and effectively judge magnitude (no truncating axis).
# 1100: Why plots? So we can co-opt our visual processing to better understand numerical ratios and effectively judge magnitude (no truncating axis).
library(tidyverse)
install.packages("purr")
install.packages("purrr")
library(tidyverse)
data(mpg)
mpg
# creating a ggplot
# The first line sets up a coordinate system.
# the second line maps displ to x, hwy to y, and draws points
ggplot(data = mpg) +
geom_point(mapping = aes(x = displ, y = hwy))
# aesthetic mappings can get more complicated:
# here we vary point color by some third variable
ggplot(data = mpg) +
geom_point(mapping = aes(x = displ, y = hwy, color = class))
# Lots of options for point characeristics that can be changed.
# Some aesthetic mappings are more effective than others!
# For example, compare the following:
# size of point
ggplot(data = mpg) +
geom_point(mapping = aes(x = displ, y = hwy, size = class))
# transparency
ggplot(data = mpg) +
geom_point(mapping = aes(x = displ, y = hwy, alpha = class))
# point shape
ggplot(data = mpg) +
geom_point(mapping = aes(x = displ, y = hwy, shape = class))
# adding a title
ggplot(mpg, aes(displ, hwy)) +
geom_point(aes(color = class)) +
labs(title = "Fuel efficiency generally decreases with engine size")
# manually setting an aesthetic property
ggplot(data = mpg) +
geom_point(mapping = aes(x = displ, y = hwy), color = "blue")
# note: compare with
ggplot(data = mpg) +
geom_point(mapping = aes(x = displ, y = hwy, color = "blue"))
# faceting on two variables
ggplot(data = mpg) +
geom_point(mapping = aes(x = displ, y = hwy)) +
facet_grid(drv ~ cyl)
# axis labels
ggplot(mpg, aes(displ, hwy)) +
geom_point(aes(colour = class)) +
geom_smooth(se = FALSE) +
labs(
x = "Engine displacement (L)",
y = "Highway fuel economy (mpg)",
colour = "Car type"
)
# axis labels
ggplot(mpg, aes(displ, hwy)) +
geom_point(aes(colour = class)) +
geom_smooth(se = FALSE) +
labs(
x = "Engine displacement (L)",
y = "Highway fuel economy (mpg)",
colour = "Car type"
)
library(tidyverse)
TitanicSurvival = read.csv('../data/TitanicSurvival.csv')
summary(TitanicSurvival)
# a bit un-interesting
ggplot(data = TitanicSurvival) +
geom_bar(mapping = aes(x = survived))
# Let's create a new data set of survival percentages
# We'll group by sex
d1 = TitanicSurvival %>%
group_by(sex) %>%
summarize(surv_pct = sum(survived=='yes')/n())
d1
TitanicSurvival = read.csv('./data/TitanicSurvival.csv')
summary(TitanicSurvival)
TitanicSurvival = read.csv('../data/TitanicSurvival.csv')
summary(TitanicSurvival)
library(tidyverse)
TitanicSurvival = read.csv('../data/TitanicSurvival.csv')
summary(TitanicSurvival)
TitanicSurvival = read.csv('.../data/TitanicSurvival.csv')
library(tidyverse)
TitanicSurvival = read.csv('../data/TitanicSurvival.csv')
TitanicSurvival = read.csv('..\data\TitanicSurvival.csv')
TitanicSurvival = read.csv('../data/TitanicSurvival.csv')
cd
wd
sd
cd
getwd
getwd()
this.dir <- dirname(parent.frame(2)$ofile)
setwd(this.dir)
install.packages("FNN")
library(tidyverse)
library(FNN)
# read in the data: make sure to use the path name to
# wherever you'd stored the file
loadhou = read.csv('C:\Dropbox\Classes\ECO 395M Data mining and statisical learning\github_clone\ECO395M\data')
library(tidyverse)
library(FNN)
# read in the data: make sure to use the path name to
# wherever you'd stored the file
loadhou = read.csv('C:/Dropbox/Classes/ECO 395M Data mining and statisical learning/github_clone/ECO395M/data')
loadhou = read.csv('C:/Dropbox/Classes/ECO 395M Data mining and statisical learning/github_clone/ECO395M/data/loadhou.csv')
summary(loadhou)
# plot the data
ggplot(data = loadhou) +
geom_point(mapping = aes(x = KHOU, y = COAST), color='darkgrey') +
ylim(7000, 20000)
# Make a train-test split
N = nrow(loadhou)
N_train = floor(0.8*N)
N_test = N - N_train
loadhou = read.csv('./github_clone/ECO395M/data/loadhou.csv')
loadhou = read.csv('../github_clone/ECO395M/data/loadhou.csv')
loadhou = read.csv('C:/Dropbox/Classes/ECO 395M Data mining and statisical learning/github_clone/ECO395M/data/loadhou.csv')
library(tidyverse)
library(FNN)
# read in the data: make sure to use the path name to
# wherever you'd stored the file
loadhou = read.csv('C:/Dropbox/Classes/ECO 395M Data mining and statisical learning/github_clone/ECO395M/data/loadhou.csv')
summary(loadhou)
# plot the data
ggplot(data = loadhou) +
geom_point(mapping = aes(x = KHOU, y = COAST), color='darkgrey') +
ylim(7000, 20000)
# Make a train-test split
N = nrow(loadhou)
N_train = floor(0.8*N)
N_test = N - N_train
#####
# Train/test split
#####
# randomly sample a set of data points to include in the training set
train_ind = sample.int(N, N_train, replace=FALSE)
# Define the training and testing set
D_train = loadhou[train_ind,]
D_test = loadhou[-train_ind,]
gwd
wd
getwd
getwd()
library(tidyverse)
library(FNN)
# read in the data: make sure to use the path name to
# wherever you'd stored the file
loadhou = read.csv('C:/Dropbox/Classes/ECO 395M Data mining and statisical learning/github_clone/ECO395M/data/loadhou.csv')
summary(loadhou)
# plot the data
ggplot(data = loadhou) +
geom_point(mapping = aes(x = KHOU, y = COAST), color='darkgrey') +
ylim(7000, 20000)
# Make a train-test split
N = nrow(loadhou)
N_train = floor(0.8*N)
N_test = N - N_train
#####
# Train/test split
#####
# randomly sample a set of data points to include in the training set
train_ind = sample.int(N, N_train, replace=FALSE)
# Define the training and testing set
D_train = loadhou[train_ind,]
D_test = loadhou[-train_ind,]
# optional book-keeping step:
# reorder the rows of the testing set by the KHOU (temperature) variable
# this isn't necessary, but it will allow us to make a pretty plot later
D_test = arrange(D_test, KHOU)
head(D_test)
# Now separate the training and testing sets into features (X) and outcome (y)
X_train = select(D_train, KHOU)
y_train = select(D_train, COAST)
X_test = select(D_test, KHOU)
y_test = select(D_test, COAST)
#####
# Fit a few models
#####
# linear and quadratic models
lm1 = lm(COAST ~ KHOU, data=D_train)
lm2 = lm(COAST ~ poly(KHOU, 2), data=D_train)
# KNN 250
knn250 = knn.reg(train = X_train, test = X_test, y = y_train, k=250)
names(knn250)
library(tidyverse)
library(FNN)
# read in the data: make sure to use the path name to
# wherever you'd stored the file
loadhou = read.csv('C:/Dropbox/Classes/ECO 395M Data mining and statisical learning/github_clone/ECO395M/data/loadhou.csv')
summary(loadhou)
# plot the data
ggplot(data = loadhou) +
geom_point(mapping = aes(x = KHOU, y = COAST), color='darkgrey') +
ylim(7000, 20000)
# Make a train-test split
N = nrow(loadhou)
N_train = floor(0.8*N)
N_test = N - N_train
#####
# Train/test split
#####
# randomly sample a set of data points to include in the training set
train_ind = sample.int(N, N_train, replace=FALSE)
# Define the training and testing set
D_train = loadhou[train_ind,]
D_test = loadhou[-train_ind,]
# optional book-keeping step:
# reorder the rows of the testing set by the KHOU (temperature) variable
# this isn't necessary, but it will allow us to make a pretty plot later
D_test = arrange(D_test, KHOU)
head(D_test)
# Now separate the training and testing sets into features (X) and outcome (y)
X_train = select(D_train, KHOU)
y_train = select(D_train, COAST)
X_test = select(D_test, KHOU)
y_test = select(D_test, COAST)
#####
# Fit a few models
#####
# linear and quadratic models
lm1 = lm(COAST ~ KHOU, data=D_train)
lm2 = lm(COAST ~ poly(KHOU, 2), data=D_train)
# KNN 250
knn250 = knn.reg(train = X_train, test = X_test, y = y_train, k=250)
names(knn250)
#####
# Compare the models by RMSE_out
#####
# define a helper function for calculating RMSE
rmse = function(y, ypred) {
sqrt(mean((y-ypred)^2))
}
ypred_lm1 = predict(lm1, X_test)
ypred_lm2 = predict(lm2, X_test)
ypred_knn250 = knn250$pred
rmse(y_test, ypred_lm1)
rmse(y_test, ypred_lm2)
rmse(y_test, ypred_knn250)
ypred_lm1 = predict(lm1, X_test)
ypred_lm2 = predict(lm2, X_test)
ypred_knn250 = knn250$pred
rmse(y_test, ypred_lm1)
rmse(y_test, ypred_lm2)
rmse(y_test, ypred_knn250)
ypred_lm1
y_test
ypred_lm1
rmse(y_test, ypred_lm1)
rmse(y_test, ypred_lm2)
rmse(y_test, ypred_knn250)
rmse = function(y, x) {
sqrt(mean((y-ypred)^2))
}
rmse(y_test, ypred_lm1)
rmse(y_test, ypred_lm2)
rmse(y_test, ypred_knn250)
ypred_lm1 = predict(lm1, X_test)
ypred_lm2 = predict(lm2, X_test)
ypred_knn250 = knn250$pred
rmse(y_test, ypred_lm1)
rmse(y_test, ypred_lm2)
rmse(y_test, ypred_knn250)
rmse = function(y, x) {
sqrt(mean((y-x)^2))
}
ypred_lm1 = predict(lm1, X_test)
ypred_lm2 = predict(lm2, X_test)
ypred_knn250 = knn250$pred
rmse(y_test, ypred_lm1)
rmse(y_test, ypred_lm2)
rmse(y_test, ypred_knn250)
y_test
ypred_lm1
rmse = function(y, ypred) {
sqrt(mean((y-as.numeric(ypred))^2))
}
ypred_lm1 = predict(lm1, X_test)
ypred_lm2 = predict(lm2, X_test)
ypred_knn250 = knn250$pred
rmse(y_test, ypred_lm1)
rmse(y_test, ypred_lm2)
rmse(y_test, ypred_knn250)
as.numeric(ypred_lm1)
rmse(y_test, ypred_lm1)
y_test
as.numeric(ypred_lm1)
data.matrix(ypred_lm1)
rmse(y_test, ypred_lm1)
rmse = function(y, ypred) {
sqrt(mean((data.matrix(y-ypred))^2))
}
ypred_lm1 = predict(lm1, X_test)
ypred_lm2 = predict(lm2, X_test)
ypred_knn250 = knn250$pred
rmse(y_test, ypred_lm1)
rmse(y_test, ypred_lm2)
rmse(y_test, ypred_knn250)
# attach the predictions to the test data frame
D_test$ypred_lm2 = ypred_lm2
D_test$ypred_knn250 = ypred_knn250
p_test = ggplot(data = D_test) +
geom_point(mapping = aes(x = KHOU, y = COAST), color='lightgrey') +
theme_bw(base_size=18) +
ylim(7000, 20000)
p_test
p_test + geom_point(aes(x = KHOU, y = ypred_knn250), color='red')
p_test + geom_path(aes(x = KHOU, y = ypred_knn250), color='red')
p_test + geom_path(aes(x = KHOU, y = ypred_knn250), color='red') +
geom_path(aes(x = KHOU, y = ypred_lm2), color='blue')
N
source('C:/Dropbox/Classes/ECO 395M Data mining and statisical learning/2019-02-06 Class/in-class exercise.R', echo=TRUE)
n
N
source('C:/Dropbox/Classes/ECO 395M Data mining and statisical learning/2019-02-06 Class/in-class exercise.R', echo=TRUE)
N
loadhou = loadhou[sample(nrow(loadhou), 150), ]
loadhou
source('C:/Dropbox/Classes/ECO 395M Data mining and statisical learning/2019-02-06 Class/in-class exercise.R', echo=TRUE)
source('C:/Dropbox/Classes/ECO 395M Data mining and statisical learning/2019-02-06 Class/in-class exercise.R', echo=TRUE)
source('C:/Dropbox/Classes/ECO 395M Data mining and statisical learning/2019-02-06 Class/in-class exercise.R', echo=TRUE)
source('C:/Dropbox/Classes/ECO 395M Data mining and statisical learning/2019-02-06 Class/in-class exercise.R', echo=TRUE)
source('C:/Dropbox/Classes/ECO 395M Data mining and statisical learning/2019-02-06 Class/in-class exercise.R', echo=TRUE)
source('C:/Dropbox/Classes/ECO 395M Data mining and statisical learning/2019-02-06 Class/in-class exercise.R', echo=TRUE)
source('C:/Dropbox/Classes/ECO 395M Data mining and statisical learning/2019-02-06 Class/in-class exercise.R', echo=TRUE)
source('C:/Dropbox/Classes/ECO 395M Data mining and statisical learning/2019-02-06 Class/in-class exercise.R', echo=TRUE)
source('C:/Dropbox/Classes/ECO 395M Data mining and statisical learning/2019-02-06 Class/in-class exercise.R', echo=TRUE)
loadhou = loadhou[sample.int(nrow(loadhou), 150), replace=FALSE]
source('C:/Dropbox/Classes/ECO 395M Data mining and statisical learning/2019-02-06 Class/in-class exercise.R', echo=TRUE)
source('C:/Dropbox/Classes/ECO 395M Data mining and statisical learning/2019-02-06 Class/in-class exercise.R', echo=TRUE)
source('C:/Dropbox/Classes/ECO 395M Data mining and statisical learning/2019-02-06 Class/in-class exercise.R', echo=TRUE)
source('C:/Dropbox/Classes/ECO 395M Data mining and statisical learning/2019-02-06 Class/in-class exercise.R', echo=TRUE)
loadhou_sample = loadhou[sample.int(nrow(loadhou), 150), ]
loadhou_sample
source('C:/Dropbox/Classes/ECO 395M Data mining and statisical learning/2019-02-06 Class/in-class exercise.R', echo=TRUE)
source('C:/Dropbox/Classes/ECO 395M Data mining and statisical learning/2019-02-06 Class/in-class exercise.R', echo=TRUE)
source('C:/Dropbox/Classes/ECO 395M Data mining and statisical learning/2019-02-06 Class/in-class exercise.R', echo=TRUE)
source('C:/Dropbox/Classes/ECO 395M Data mining and statisical learning/2019-02-06 Class/in-class exercise.R', echo=TRUE)
brca <- read.csv('brca.csv')
setwd("C:/Dropbox/Classes/ECO 395M Data mining and statisical learning/2019-02-20 Class")
brca <- read.csv('brca.csv')
source('C:/Dropbox/Classes/ECO 395M Data mining and statisical learning/eco395m_team_awesome/Homework 2/question 2.R', echo=TRUE)
brca <- read.csv('./brca.csv')
brca <- read.csv('../brca.csv')
brca <- read.csv("./brca.csv")
setwd("C:/Dropbox/Classes/ECO 395M Data mining and statisical learning/eco395m_team_awesome/Homework 2")
brca <- read.csv("./brca.csv")
source('C:/Dropbox/Classes/ECO 395M Data mining and statisical learning/eco395m_team_awesome/Homework 2/question 2.R', echo=TRUE)
source('C:/Dropbox/Classes/ECO 395M Data mining and statisical learning/eco395m_team_awesome/Homework 2/question 2.R', echo=TRUE)
df = multinom(cancer ~ .-recall,
data=brca,
maxit = maxit)
df
df = multinom(cancer ~ .-recall,
data=brca,
maxit = 2000)
df = multinom(cancer ~ .-recall,
data=brca,
maxit = 1000)
df
ml1 = glm(recall ~ .-cancer,
data=brca_train,
maxit = maxit)
ml2 = glm(recall ~ (.-cancer)^2,
data=brca_train,
maxit = maxit)
glm
ml1
count(brca$radiologist==radiologist89)
count(brca$radiologist=="radiologist89")
count(brca$radiologist=="radiologist95")
197*5
count(brca$radiologist=="radiologist34")
count(brca$radiologist=="radiologist66")
colMeans(do_many)
names(brca)
source('C:/Dropbox/Classes/ECO 395M Data mining and statisical learning/eco395m_team_awesome/Homework 2/question 2.R', echo=TRUE)
source('C:/Dropbox/Classes/ECO 395M Data mining and statisical learning/eco395m_team_awesome/Homework 2/question 2.R', echo=TRUE)
source('C:/Dropbox/Classes/ECO 395M Data mining and statisical learning/eco395m_team_awesome/Homework 2/question 2.R', echo=TRUE)
source('C:/Dropbox/Classes/ECO 395M Data mining and statisical learning/eco395m_team_awesome/Homework 2/question 2.R', echo=TRUE)
brca <- read.csv("./brca.csv")
source('C:/Dropbox/Classes/ECO 395M Data mining and statisical learning/eco395m_team_awesome/Homework 2/question 2.R', echo=TRUE)
# split train and test set
n = nrow(brca) #count all of the rows in the df
n_train = round(0.8*n) #take 0.8 times n, then round to nearest
train_ind = sort(sample.int(n, n_train, replace=FALSE)) #randomly sample from n, n_train times, without replacement, then sort
n_test = n - n_train #number of n test is n minus n_train
brca_train = brca[train_ind,] #create a dataframe of just the training
brca_test = brca[-train_ind,] #create a dataframe of the just the test
# train models: recall
maxit = 10000
ml1 = glm(recall ~ .-cancer,
data=brca_train,
maxit = maxit)
ml2 = glm(recall ~ (.-cancer)^2,
data=brca_train,
maxit = maxit)
# train models: cancer
ml3 = glm(cancer ~ recall,
data=brca_train,
maxit = maxit)
ml4 = glm(cancer ~ recall + history,
data=brca_train,
maxit = maxit)
ml5 = glm(cancer ~ .,
data=brca_train,
maxit = maxit)
# make predictions
predict_test = function(x){
predict(x, newdata=brca_test, type='probs')
}
probhat1_test = predict_test(ml1)
probhat2_test = predict_test(ml2)
function (arg, choices, several.ok = FALSE)
