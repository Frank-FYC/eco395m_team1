help(sum)
source('C:/Dropbox/Classes/ECO 395M Data mining and statisical learning/eco395m_team_awesome/Homework 2/question 2.R', echo=TRUE)
cbind(probhat1_test, brca_test$cancer)
source('C:/Dropbox/Classes/ECO 395M Data mining and statisical learning/eco395m_team_awesome/Homework 2/question 2.R', echo=TRUE)
cbind(probhat2_test, brca_test$cancer)
sum(diag(conf2))/n_test
conf2 = table(brca_test$cancer, yhat2_test)
sum(diag(conf1))/n_test
ml2
ml2
ml2
ml3
ml1
unique(brca$radiologist)
ml5 = multinom(recall ~ (.-cancer)^2,
data=brca_train,
maxit = maxit)
ml5
ml5 = multinom(cancer ~ .-cancer,
data=brca_train,
maxit = maxit)
reg recall radiology, ml5
ml5 = multinom(cancer ~ .-cancer,
data=brca_train,
maxit = maxit)
ml5
ml5 = multinom(recall ~ .-cancer,
data=brca_train,
maxit = maxit)
ml5
unique(brca$radiologist)
-3.09893533+0.14349902+-0.20364080+1.25528898
# make predictions
predict_test = function(x){
predict(x, newdata=brca_test, type='probs')
}
probhat1_test = predict_test(ml1)
ml1
probhat2_test = predict_test(ml2)
sum(diag(conf1))/n_test
sum(diag(conf2))/n_test
sum(diag(conf3))/n_test
conf2
conf3
conf4
knitr::opts_chunk$set(echo = TRUE)
library(MASS) 	## a library of example datasets
library(tidyverse)
library(nnet)  # for multinom
library(MASS) 	## a library of example datasets
library(tidyverse)
library(nnet)  # for multinom
brca <- read.csv("./brca.csv")
library(MASS) 	## a library of example datasets
library(tidyverse)
library(nnet)  # for multinom
brca <- read.csv('./brca.csv')
knitr::opts_chunk$set(echo = TRUE)
library(MASS) 	## a library of example datasets
library(tidyverse)
library(nnet)  # for multinom
this.dir <- dirname(parent.frame(2)$ofile)
source('C:/Dropbox/Classes/ECO 395M Data mining and statisical learning/eco395m_team_awesome/Homework 2/question 2.R', echo=TRUE)
source('C:/Dropbox/Classes/ECO 395M Data mining and statisical learning/eco395m_team_awesome/Homework 2/question 2.R', echo=TRUE)
knitr::opts_chunk$set(echo = TRUE)
library(MASS) 	## a library of example datasets
library(tidyverse)
library(nnet)  # for multinom
this.dir <- dirname(parent.frame(2)$ofile)
setwd("C:/Dropbox/Classes/ECO 395M Data mining and statisical learning/eco395m_team_awesome/Homework 1")
knitr::opts_chunk$set(echo = TRUE)
library(MASS) 	## a library of example datasets
library(tidyverse)
library(nnet)  # for multinom
library(MASS) 	## a library of example datasets
library(tidyverse)
library(nnet)  # for multinom
brca <- read.csv('./brca.csv')
knitr::opts_chunk$set(echo = TRUE)
library(MASS) 	## a library of example datasets
library(tidyverse)
library(nnet)  # for multinom
library(MASS) 	## a library of example datasets
library(tidyverse)
library(nnet)  # for multinom
brca <- read.csv('../brca.csv')
brca <- read.csv("../brca.csv")
brca <- read.csv("./brca.csv")
source('C:/Dropbox/Classes/ECO 395M Data mining and statisical learning/eco395m_team_awesome/Homework 2/question 2.R', echo=TRUE)
library(MASS) 	## a library of example datasets
library(tidyverse)
library(nnet)  # for multinom
brca <- read.csv('./brca.csv')
# First question: are some radiologists more clinically conservative than others in recalling patients, holding patient risk factors equal?
# Some advice: imagine two radiologists who see the mammogram of a single patient, who has a specific set of risk factors. If radiologist A has a higher probability of recalling that patient than radiologist B, we’d say that radiologist A is more conservative (because they have a lower threshold for wanting to double-check the patient’s results). So if all five radiologists saw the same set of patients, we’d easily find out whether some radiologists are more conservative than others. The problem is that the radiologists don’t see the same patients. So we can’t just look at raw recall rates—some radiologists might have seen patients whose clinical situation mandated more conservatism in the first place. Can you build a classification model that addresses this problem, i.e. that holds risk factors constant in assessing whether some radiologists are more conservative than others in recalling patients?
# split train and test set
n = nrow(brca) #count all of the rows in the df
n_train = round(0.8*n) #take 0.8 times n, then round to nearest
train_ind = sort(sample.int(n, n_train, replace=FALSE)) #randomly sample from n, n_train times, without replacement, then sort
n_test = n - n_train #number of n test is n minus n_train
brca_train = brca[train_ind,] #create a dataframe of just the training
brca_test = brca[-train_ind,] #create a dataframe of the just the test
# train three multinomial logit models
maxit = 10000
ml1 = multinom(cancer ~ radiologist,
data=brca_train,
maxit = maxit)
ml2 = multinom(cancer ~ radiologist + recall + history + symptoms,
data=brca_train,
maxit = maxit)
ml3 = multinom(cancer ~ .,
data=brca_train,
maxit = maxit)
ml4 = multinom(cancer ~ (.)^2,
data=brca_train,
maxit = maxit)
ml5 = multinom(recall ~ .-cancer,
data=brca_train,
maxit = maxit)
ml6 = multinom(recall ~ (.-cancer)^2,
data=brca_train,
maxit = maxit)
# make predictions
predict_test = function(x){
predict(x, newdata=brca_test, type='probs')
}
probhat1_test = predict_test(ml1)
probhat2_test = predict_test(ml2)
probhat3_test = predict_test(ml3)
probhat4_test = predict_test(ml4)
# here's a generic function for calculating out-of-sample deviance
dev_out = function(y, probhat) {
rc_pairs = cbind(seq_along(y), y)
-2*sum(log(probhat[rc_pairs]))
}
# Calculate deviance
dev_out(brca_test$cancer, probhat1_test)
dev_out(brca_test$cancer, probhat2_test)
dev_out(brca_test$cancer, probhat3_test)
dev_out(brca_test$cancer, probhat4_test)
# out of sample classification error rate
yhat1_test = predict(ml1, newdata=brca_test, type='class')
conf1 = table(brca_test$cancer, yhat1_test)
sum(diag(conf1))/n_test
yhat2_test = predict(ml2, newdata=brca_test, type='class')
conf2 = table(brca_test$cancer, yhat2_test)
sum(diag(conf2))/n_test
yhat3_test = predict(ml3, newdata=brca_test, type='class')
conf3 = table(brca_test$cancer, yhat3_test)
sum(diag(conf3))/n_test
yhat4_test = predict(ml4, newdata=brca_test, type='class')
conf4 = table(brca_test$cancer, yhat4_test)
sum(diag(conf4))/n_test
cbind(probhat1_test, brca_test$cancer)
cbind(probhat2_test, brca_test$cancer)
cbind(probhat3_test, brca_test$cancer)
cbind(probhat4_test, brca_test$cancer)
ml5 = multinom(recall ~ .-cancer,
data=brca_train,
maxit = maxit)
ml8 = multinom(recall ~ (.)^2,
data=brca_train,
maxit = maxit)
ml7 = multinom(recall ~ .,
data=brca_train,
maxit = maxit)
mlml5
ml5
ml7
knitr::opts_chunk$set(echo = TRUE)
library(MASS) 	## a library of example datasets
library(tidyverse)
library(nnet)  # for multinom
library(MASS) 	## a library of example datasets
library(tidyverse)
library(nnet)  # for multinom
brca <- read.csv('./brca.csv')
# First question: are some radiologists more clinically conservative than others in recalling patients, holding patient risk factors equal?
# Some advice: imagine two radiologists who see the mammogram of a single patient, who has a specific set of risk factors. If radiologist A has a higher probability of recalling that patient than radiologist B, we’d say that radiologist A is more conservative (because they have a lower threshold for wanting to double-check the patient’s results). So if all five radiologists saw the same set of patients, we’d easily find out whether some radiologists are more conservative than others. The problem is that the radiologists don’t see the same patients. So we can’t just look at raw recall rates—some radiologists might have seen patients whose clinical situation mandated more conservatism in the first place. Can you build a classification model that addresses this problem, i.e. that holds risk factors constant in assessing whether some radiologists are more conservative than others in recalling patients?
# split train and test set
n = nrow(brca) #count all of the rows in the df
n_train = round(0.8*n) #take 0.8 times n, then round to nearest
train_ind = sort(sample.int(n, n_train, replace=FALSE)) #randomly sample from n, n_train times, without replacement, then sort
n_test = n - n_train #number of n test is n minus n_train
brca_train = brca[train_ind,] #create a dataframe of just the training
brca_test = brca[-train_ind,] #create a dataframe of the just the test
# train three multinomial logit models
maxit = 10000
ml1 = multinom(recall ~ radiologist,
data=brca_train,
maxit = maxit)
ml2 = multinom(recall ~ radiologist + history + symptoms,
data=brca_train,
maxit = maxit)
ml3 = multinom(recall ~ .,
data=brca_train,
maxit = maxit)
ml4 = multinom(recall ~ (.)^2,
data=brca_train,
maxit = maxit)
ml5 = multinom(recall ~ .-cancer,
data=brca_train,
maxit = maxit)
ml6 = multinom(recall ~ (.-cancer)^2,
data=brca_train,
maxit = maxit)
# make predictions
predict_test = function(x){
predict(x, newdata=brca_test, type='probs')
}
probhat1_test = predict_test(ml1)
probhat2_test = predict_test(ml2)
probhat3_test = predict_test(ml3)
probhat4_test = predict_test(ml4)
# here's a generic function for calculating out-of-sample deviance
dev_out = function(y, probhat) {
rc_pairs = cbind(seq_along(y), y)
-2*sum(log(probhat[rc_pairs]))
}
# Calculate deviance
dev_out(brca_test$cancer, probhat1_test)
dev_out(brca_test$cancer, probhat2_test)
dev_out(brca_test$cancer, probhat3_test)
dev_out(brca_test$cancer, probhat4_test)
# out of sample classification error rate
yhat1_test = predict(ml1, newdata=brca_test, type='class')
conf1 = table(brca_test$cancer, yhat1_test)
sum(diag(conf1))/n_test
yhat2_test = predict(ml2, newdata=brca_test, type='class')
conf2 = table(brca_test$cancer, yhat2_test)
sum(diag(conf2))/n_test
yhat3_test = predict(ml3, newdata=brca_test, type='class')
conf3 = table(brca_test$cancer, yhat3_test)
sum(diag(conf3))/n_test
yhat4_test = predict(ml4, newdata=brca_test, type='class')
conf4 = table(brca_test$cancer, yhat4_test)
sum(diag(conf4))/n_test
cbind(probhat1_test, brca_test$cancer)
cbind(probhat2_test, brca_test$cancer)
cbind(probhat3_test, brca_test$cancer)
cbind(probhat4_test, brca_test$cancer)
ml3 = multinom(cancer ~ recall,
data=brca_train,
maxit = maxit)
ml4 = multinom(cancer ~ recall^2,
data=brca_train,
maxit = maxit)
ml3
ml4
ml4 = multinom(cancer ~ (recall)^2,
data=brca_train,
maxit = maxit)
ml4
class_err = function(x){
table(brca_test$cancer,
predict(x, newdata=brca_test, type='class')
)
}
conf1 = class_err(ml1)
conf1
yhat1_test = predict(ml1, newdata=brca_test, type='class')
conf1 = table(brca_test$cancer, yhat1_test)
sum(diag(conf1))/n_test
class_err = function(x){
table(brca_test$cancer,
predict(x, newdata=brca_test, type='class')
)
}
conf1 = class_err(ml1)
sum(diag(conf1))/n_test
conf1 = class_err(ml1)
conf2 = class_err(ml2)
conf3 = class_err(ml3)
conf4 = class_err(ml4)
conf5 = class_err(ml5)
conf5
conf4
yhat4_test = predict(ml4, newdata=brca_test, type='class')
conf4 = table(brca_test$cancer, yhat4_test)
conf4
source('C:/Dropbox/Classes/ECO 395M Data mining and statisical learning/eco395m_team_awesome/Homework 2/question 2.R', echo=TRUE)
ml3 = multinom(cancer ~ recall,
data=brca_train,
maxit = maxit)
ml4 = multinom(cancer ~ .,
data=brca_train,
maxit = maxit)
ml5 = multinom(cancer ~ (.)^2,
data=brca_train,
maxit = maxit)
ml3
ml4
ml5
source('C:/Dropbox/Classes/ECO 395M Data mining and statisical learning/eco395m_team_awesome/Homework 2/question 2.R', echo=TRUE)
conf3
conf4
conf5
class_err = function(x){
yhat_test = predict(x, newdata=brca_test, type='class')
table(brca_test$cancer,yhat_test)
)
}
conf3 = class_err(ml3)
class_err = function(x){
yhat_test = predict(x, newdata=brca_test, type='class')
table(brca_test$cancer,yhat_test)
}
conf3 = class_err(ml3)
conf3
conf4
class_err = function(x){
yhat_test = predict(x, newdata=brca_test, type='class')
table(brca_test$cancer,yhat_test)
}
conf3 = class_err(ml3)
conf4 = class_err(ml4)
conf5 = class_err(ml5)
sum(diag(conf3))/n_test
sum(diag(conf4))/n_test
sum(diag(conf5))/n_test
conf3
yhat4_test
sum(diag(conf5))/n_test
source('C:/Dropbox/Classes/ECO 395M Data mining and statisical learning/eco395m_team_awesome/Homework 2/question 2.R', echo=TRUE)
do_many = do(100)*{
# split train and test set
n = nrow(brca) #count all of the rows in the df
n_train = round(0.8*n) #take 0.8 times n, then round to nearest
train_ind = sort(sample.int(n, n_train, replace=FALSE)) #randomly sample from n, n_train times, without replacement, then sort
n_test = n - n_train #number of n test is n minus n_train
brca_train = brca[train_ind,] #create a dataframe of just the training
brca_test = brca[-train_ind,] #create a dataframe of the just the test
# train models: recall
maxit = 10000
ml1 = multinom(recall ~ .-cancer,
data=brca_train,
maxit = maxit)
ml2 = multinom(recall ~ (.-cancer)^2,
data=brca_train,
maxit = maxit)
# train models: cancer
ml3 = multinom(cancer ~ recall,
data=brca_train,
maxit = maxit)
ml4 = multinom(cancer ~ .,
data=brca_train,
maxit = maxit)
ml5 = multinom(cancer ~ (.)^2,
data=brca_train,
maxit = maxit)
# make predictions
predict_test = function(x){
predict(x, newdata=brca_test, type='probs')
}
probhat1_test = predict_test(ml1)
probhat2_test = predict_test(ml2)
probhat3_test = predict_test(ml3)
probhat4_test = predict_test(ml4)
probhat5_test = predict_test(ml5)
# here's a generic function for calculating out-of-sample deviance
dev_out = function(y, probhat) {
rc_pairs = cbind(seq_along(y), y)
-2*sum(log(probhat[rc_pairs]))
}
# Calculate deviance
dev_out(brca_test$cancer, probhat1_test)
dev_out(brca_test$cancer, probhat2_test)
dev_out(brca_test$cancer, probhat3_test)
dev_out(brca_test$cancer, probhat4_test)
dev_out(brca_test$cancer, probhat5_test)
# out of sample classification error rate
yhat_test = function(x){
predict(x, newdata=brca_test, type='class')
}
yhat3_test = yhat_test(ml3)
yhat4_test = yhat_test(ml4)
yhat5_test = yhat_test(ml5)
class_err = function(x){
table(brca_test$cancer,x)
}
conf3 = class_err(yhat3_test)
conf4 = class_err(yhat4_test)
conf5 = class_err(yhat5_test)
sum(diag(conf3))/n_test
sum(diag(conf4))/n_test
sum(diag(conf5))/n_test
c(brca_test$cancer,
sum(diag(conf3))/n_test,
sum(diag(conf4))/n_test,
sum(diag(conf5))/n_test
)
do_many
colmeans(do_many)
#cbind(probhat3_test, brca_test$cancer)
#cbind(probhat4_test, brca_test$cancer)
#cbind(probhat5_test, brca_test$cancer)
}
source('C:/Dropbox/Classes/ECO 395M Data mining and statisical learning/eco395m_team_awesome/Homework 2/question 2.R', echo=TRUE)
colMeans(do_many)
do_many
view(do_many)
sum(diag(conf3))/n_test
brca_test$cancer
View(do_many)
source('C:/Dropbox/Classes/ECO 395M Data mining and statisical learning/2019-02-20 Class/saratoga_lm.R', echo=TRUE)
View(rmse_vals)
source('C:/Dropbox/Classes/ECO 395M Data mining and statisical learning/eco395m_team_awesome/Homework 2/question 2.R', echo=TRUE)
source('C:/Dropbox/Classes/ECO 395M Data mining and statisical learning/eco395m_team_awesome/Homework 2/question 2.R', echo=TRUE)
source('C:/Dropbox/Classes/ECO 395M Data mining and statisical learning/eco395m_team_awesome/Homework 2/question 2.R', echo=TRUE)
names(do_many) = c("ml3","ml4","ml5")
colMeans(do_many)
clear all
clear()
rm()
rm(all)
rm(list=ls())
lapply(paste('package:',names(sessionInfo()$otherPkgs),sep=""),detach,character.only=TRUE,unload=TRUE)
knitr::opts_chunk$set(echo = TRUE)
rm(list=ls())
library(MASS) 	## a library of example datasets
library(tidyverse)
library(nnet)  # for multinom
library(mosaic)
this.dir <- dirname(parent.frame(2)$ofile)
rm(list=ls())
library(MASS) 	## a library of example datasets
library(tidyverse)
library(nnet)  # for multinom
library(mosaic)
this.dir <- dirname(parent.frame(2)$ofile)
knitr::opts_chunk$set(echo = TRUE)
rm(list=ls())
library(MASS) 	## a library of example datasets
library(tidyverse)
library(nnet)  # for multinom
library(mosaic)
brca <- read.csv("./brca.csv")
# First question: are some radiologists more clinically conservative than others in recalling patients, holding patient risk factors equal?
# Some advice: imagine two radiologists who see the mammogram of a single patient, who has a specific set of risk factors. If radiologist A has a higher probability of recalling that patient than radiologist B, we’d say that radiologist A is more conservative (because they have a lower threshold for wanting to double-check the patient’s results). So if all five radiologists saw the same set of patients, we’d easily find out whether some radiologists are more conservative than others. The problem is that the radiologists don’t see the same patients. So we can’t just look at raw recall rates—some radiologists might have seen patients whose clinical situation mandated more conservatism in the first place. Can you build a classification model that addresses this problem, i.e. that holds risk factors constant in assessing whether some radiologists are more conservative than others in recalling patients?
do_many = do(100)*{
# split train and test set
n = nrow(brca) #count all of the rows in the df
n_train = round(0.8*n) #take 0.8 times n, then round to nearest
train_ind = sort(sample.int(n, n_train, replace=FALSE)) #randomly sample from n, n_train times, without replacement, then sort
n_test = n - n_train #number of n test is n minus n_train
brca_train = brca[train_ind,] #create a dataframe of just the training
brca_test = brca[-train_ind,] #create a dataframe of the just the test
# train models: recall
maxit = 10000
ml1 = multinom(recall ~ .-cancer,
data=brca_train,
maxit = maxit)
ml2 = multinom(recall ~ (.-cancer)^2,
data=brca_train,
maxit = maxit)
# train models: cancer
ml3 = multinom(cancer ~ recall,
data=brca_train,
maxit = maxit)
ml4 = multinom(cancer ~ .,
data=brca_train,
maxit = maxit)
ml5 = multinom(cancer ~ (.)^2,
data=brca_train,
maxit = maxit)
# make predictions
predict_test = function(x){
predict(x, newdata=brca_test, type='probs')
}
probhat1_test = predict_test(ml1)
probhat2_test = predict_test(ml2)
probhat3_test = predict_test(ml3)
probhat4_test = predict_test(ml4)
probhat5_test = predict_test(ml5)
# here's a generic function for calculating out-of-sample deviance
dev_out = function(y, probhat) {
rc_pairs = cbind(seq_along(y), y)
-2*sum(log(probhat[rc_pairs]))
}
# Calculate deviance
dev_out(brca_test$cancer, probhat1_test)
dev_out(brca_test$cancer, probhat2_test)
dev_out(brca_test$cancer, probhat3_test)
dev_out(brca_test$cancer, probhat4_test)
dev_out(brca_test$cancer, probhat5_test)
# out of sample classification error rate
yhat_test = function(x){
predict(x, newdata=brca_test, type='class')
}
yhat3_test = yhat_test(ml3)
yhat4_test = yhat_test(ml4)
yhat5_test = yhat_test(ml5)
class_err = function(x){
table(brca_test$cancer,x)
}
conf3 = class_err(yhat3_test)
conf4 = class_err(yhat4_test)
conf5 = class_err(yhat5_test)
sum(diag(conf3))/n_test
sum(diag(conf4))/n_test
sum(diag(conf5))/n_test
c(sum(diag(conf3))/n_test,
sum(diag(conf4))/n_test,
sum(diag(conf5))/n_test
)
#cbind(probhat3_test, brca_test$cancer)
#cbind(probhat4_test, brca_test$cancer)
#cbind(probhat5_test, brca_test$cancer)
}
do_many
names(do_many) = c("ml3","ml4","ml5")
colMeans(do_many)
