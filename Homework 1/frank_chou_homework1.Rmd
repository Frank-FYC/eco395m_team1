---
title: 'Data Mining and Statistical Learning: Exercise 1'
author: "Frank Chou"
date: "February 9, 2019"
output:
  pdf_document: default
  html_document:
    df_print: paged
indent: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Question 1

```{r q1 load data, include=FALSE}
library(tidyverse)
library(reshape2)
library(expss)

df <- read.csv('greenbuildings.csv')

# Replicating the Analyst's results
df_rm10 <- df[df$leasing_rate > 10,] #this removes rows where leasing_rate is < 10
df_rm10_green <- df[df$green_rating == 1, ]
df_rm10_ngreen <- df[df$green_rating != 1, ]

rent <- median (df_rm10$Rent) #rent of all buildings
g_rent <- median (df_rm10_green$Rent) #rent of >10 occupancy green buildings
ng_rent <- median (df_rm10_ngreen$Rent) #rent of >10 occupancy non-green buildings
d_rent <- g_rent-ng_rent #difference in rent from green and non-green

n <- 30 #years of operation
o_rate <- 0.90 #occupancy rate
sqft <- 250000 #square footage of building
c_cost <- 100000000 #cost of building
g_cert <- 0.05 #percentage to certify green

#payoff analysis
p1 <- (c_cost*g_cert)/(d_rent*sqft) #at 100% occupancy
p2 <- (c_cost*g_cert)/(d_rent*sqft*o_rate) #at 90% occupancy

t1 <- (n-p1)*d_rent*sqft #total payoff under senario 1
t2 <- (n-p2)*d_rent*sqft*o_rate #total payoff under senario 2

full_rate <- c(100:1)/100 #a range of values from 0-100 for occupancy rates
full_payoff <- (c_cost*g_cert)/(d_rent*sqft*full_rate) #pay off times based on occupancy rates

df_plot <- data.frame(full_payoff,full_rate)

analyst_plot1 <- ggplot(df_plot)+
  geom_point(mapping = aes(x = full_payoff, y = full_rate), color='black')+
  labs(title="Payoff Analysis",x="Years to Payoff",y="Occupancy Rate")
```

# The Analyst's Analysis

```{r q1 plot 1, echo=FALSE}
analyst_plot1
```

  A rudimentary analysis of Analyst's parameters gives us a basic understanding of how long it would take to pay off the initial investment of building a green building: from a low of 7.6 years at 100% occupancy rate to over 800 years if we have a 1% occupancy rate. However, this is unrealistic, becuase if we were analyze the entire data set, we find that the rate of occupancy for green to non-green buildings are significantly different. But before we continue, let us focus on the actual building operational lifecycle of 30 years.

```{r q1 plot 2, echo=FALSE}
analyst_plot2 <-ggplot(df_plot)+
  geom_point(mapping = aes(x = full_payoff, y = full_rate), color='black')+
  labs(title="Payoff Analysis",subtitle="30 Year Operational Life",x="Years to Payoff",y="Occupancy Rate")+
  coord_cartesian(xlim=c(0, 30))+
  geom_hline(yintercept=1, linetype="dashed", color = "red")+
  geom_vline(xintercept=7.6, linetype="dashed", color = "red")+
  geom_hline(yintercept=0.25, linetype="dashed", color = "red")+
  geom_vline(xintercept=30, linetype="dashed", color = "red")
analyst_plot2
```

  Here we see a more accurate estimate of the payoff timetable for the building if we built it green. If we have 100% occupancy, we expect to pay it off within 7.6 years, however if we see 25% occupacy rates, we would break-even at the end of the building's estimated operational lifecycle, and any lower than 25% we would never break-even.
\newline
  Regardless of the analysis there are a number of points of fault within the Analyst's analysis. 

* One: by eliminating buildings where its occupancy rates are below 10%, we exclude important data that would give us insight as to how the market is doing, namely the fact that we don't expect occupancy rates to remain constant nor at max-capacity at the onsent of the building's commission. Becuase the data does not give us longitudial information regarding a given building's occupancy rate over the time of its lifetime, we do not have an idea of how a building going from non-green to green affects its rental value. 

* Two: by utilizing the median rental value of the subsets: green and non-green, we skew the data because we have more green versus non-green buildings:

```{r q1 table 1,echo=FALSE}
point2 <- table(df$green_rating)
point2
```

Where "0" represent the number of non-green buildings and "1" represent the number of green buildings

* Three: As for the decision to utilize the median instead of the mean value of rent, a cursory assessment of different values provide different results of a green vs. non-green building.

```{r q1 hist, echo=FALSE}
hist(df_rm10$Rent,xlab="Rent",main ="Histogram of Rental Prices")
``` 


  A histogram of the Analyst's data set provides a view of how the rental prices are positively skewed. There are more buildings with rental prices in the 0-50 range than >50. Using the median value prices would have a result less than the mean value.
  
```{r q1 table 2, echo=FALSE}
table <- matrix(c(
  median(df_rm10$Rent),
  mean(df_rm10$Rent)
    ),ncol=2,byrow=TRUE)

colnames(table) <- c("Median","Mean")
rownames(table) <- c("Whole Dataset")

table <- as.table(table)
table
```

  However, if we were to compare the price differences within the green versus non-green building subsets, we see that the effect of green buildings are less than anticipated. Intead of a 2.6 difference using median-values, it will only be 1.7. This will greatly extend our anticipated break-even point.
  
```{r q1 table 3, echo=FALSE}
table <- matrix(c(
  median(df_rm10$Rent),
  mean(df_rm10$Rent),
  median(df_rm10_green$Rent),
  mean(df_rm10_green$Rent),
  median(df_rm10_ngreen$Rent),
  mean(df_rm10_ngreen$Rent)
),ncol=2,byrow=TRUE)

colnames(table) <- c("Median","Mean")
rownames(table) <- c("Whole Dataset","Green","Non-Green")

table <- as.table(table)
table
```

\newpage

# A Better Approach

  If we were to conduct the analysis with the full set of data and the mean-value, we see a different picture. 
  
```{r q1 better approach, echo=FALSE}
df <- read.csv('greenbuildings.csv')
df_green <- df[df$green_rating == 1, ]
df_ngreen <- df[df$green_rating != 1, ]

rent <- mean (df$Rent) #rent of all buildings
g_rent <- mean (df_green$Rent) #rent of occupancy green buildings
ng_rent <- mean (df_ngreen$Rent) #rent of occupancy non-green buildings
d_rent <- g_rent-ng_rent #difference in rent from green and non-green

n <- 30 #years of operation
o_rate <- 0.90 #occupancy rate
sqft <- 250000 #square footage of building
c_cost <- 100000000 #cost of building
g_cert <- 0.05 #percentage to certify green

#payoff analysis
p1 <- (c_cost*g_cert)/(d_rent*sqft) #at 100% occupancy
p2 <- (c_cost*g_cert)/(d_rent*sqft*o_rate) #at 90% occupancy

t1 <- (n-p1)*d_rent*sqft #total payoff under senario 1
t2 <- (n-p2)*d_rent*sqft*o_rate #total payoff under senario 2

full_rate <- c(100:1)/100 #a range of values from 0-100 for occupancy rates
full_payoff <- (c_cost*g_cert)/(d_rent*sqft*full_rate) #pay off times based on occupancy rates

df_plot <- data.frame(full_payoff,full_rate)

# Begin Analysis of the better data

analyst_plot2 <-ggplot(df_plot)+
  geom_point(mapping = aes(x = full_payoff, y = full_rate), color='black')+
  labs(title="Payoff Analysis",subtitle="30 Year Operational Life",x="Years to Payoff",y="Occupancy Rate")+
  coord_cartesian(xlim=c(0, 30))+
  geom_hline(yintercept=1, linetype="dashed", color = "red")+
  geom_vline(xintercept=p1, linetype="dashed", color = "red")+
  geom_hline(yintercept=0.37, linetype="dashed", color = "red")+
  geom_vline(xintercept=30, linetype="dashed", color = "red")
analyst_plot2
```

  Here we see a more accurate estimate of the payoff timetable for the building if we built it green using mean-values. If we have 100% occupancy, we expect to pay it off within 11.43 years, however if we see 37% occupacy rates, we would break-even at the end of the building's estimated operational lifecycle, and any lower than 37% we would never break-even. In conclusion, we see that a more accurate assessment of the viability of building green is represented by using the full data set with mean-values instead of a truncated data set with median-values.

\newpage

## Question 2

```{r q2 load the data, include=FALSE}
library(plyr)
library(ggplot2)
library(maptools)
library(maps)
library(mapproj)
library(usmap)


abia <- read.csv('ABIA.csv')
airports <- read.csv('airports.csv')

# Let's visualize

abia_1 <- select(abia,Origin,Dest)
airports <- select(airports,iata_code,name,latitude_deg,longitude_deg)

abia_1 <- count(abia_1, vars = c("Origin", "Dest"))

origin_xy <- merge(abia_1, airports, by.x="Origin", by.y="iata_code")
names(origin_xy) <- c("origin", "destination","trips", "o_name", "oX", "oY")

dest_xy <-  merge(origin_xy, airports, by.x="destination", by.y="iata_code")
names(dest_xy) <- c("origin", "destination","trips", "o_name", "oY", "oX","d_name", "dY", "dX")

dest_xy <- dest_xy[- grep("Erase", dest_xy$o_name),]#erase the damn island of nowhere
dest_xy <- dest_xy[- grep("Erase", dest_xy$d_name),]#erase the damn island of nowhere

xquiet<- scale_x_continuous("", breaks=NULL)
yquiet<-scale_y_continuous("", breaks=NULL)
quiet<-list(xquiet, yquiet)

usa <- map_data("usa") # we already did this, but we can do it again
state <- map_data("state")

plot_1 <- ggplot()+
  geom_polygon(data = usa, aes(x=long, y = lat, group = group), fill = NA, color = "black")+
  geom_polygon(data = state,aes(x = long, y = lat, fill = region, group = group), color = "black")+
  guides(fill=FALSE)+  # do this to leave off the color legend
  geom_segment(data=dest_xy,aes(x=oX, y=oY,xend=dX, yend=dY, alpha=trips), col="black")+
  scale_alpha_continuous(range = c(0.09, 0.9))+
  theme(panel.background = element_rect(fill='white',colour='white'))+
  quiet+
  coord_fixed(1.3)+
  geom_point(data=dest_xy,aes(x=oX, y=oY) ,color="blue", size=1)+
  geom_point(data=dest_xy,aes(x=dX, y=dY) ,color="red", size=1)+
  ggtitle("Flights to and from Austin International Airport in 2008")+
  theme(plot.title = element_text(hjust = 0.5))
```
# A flight to Austin

  We can get a better understanding of which cities are connected to Austin by air. By plotting all of the flights to and from Austin to other cities in the United States, we can see which cities fly more to and from Austin. 
```{r q2 plot 1,echo=FALSE}
plot_1
```
  
\newpage

```{r q2 histogram, echo=FALSE}
# distance histogram
hist(abia$Distance,
     main = 'Distribution of flight distances',
     xlab = 'Distance [miles]',
     col = 'skyblue',
     border = 'skyblue4')
#I add a line indicating the mean of the group.
abline(v=mean(abia$Distance), 
       col = 'red',
       lwd = 3)
# I add a line indicating the median of the group.
abline(v= median(abia$Distance), 
       col = 'blue',
       lty = 5,
       lwd = 3)
legend('topright',
       legend = c('mean', 'median'), 
       lty = c(1,5),
       lwd = c(3,3),
       col = c('red', 'blue'))
```
  
  Here we have a histogram of all of the flights originating and destination from AUS, most distances are within 500 miles.

\newpage

```{r q2 plot 2, echo=FALSE}
plot_2 <- plot(abia$Distance, 
               abia$ArrDelay, 
               xlab = 'Distance [miles]', 
               ylab = 'Arrival Delay [min]', 
               main = 'Relationship between Distance and Arrival Delay', 
               pch = 20, 
               col = 'purple')
```

  Although difficult to tell with the naked eye, there is a marked increase in delay time when the flight distance increases based on the density of the plot near the 1000 mile range.

\newpage

```{r q2 plot 3, echo=FALSE}
# Seasonal delays

abia$Season <- abia$Month

#recoding the spring months
abia$Season [abia$Season == 3] <- 100
abia$Season [abia$Season == 4] <- 100
abia$Season [abia$Season == 5] <- 100

#recoding the summer months
abia$Season [abia$Season == 6] <- 200
abia$Season [abia$Season == 7] <- 200
abia$Season [abia$Season == 8] <- 200

#recoding the fall months 
abia$Season [abia$Season == 9] <- 300
abia$Season [abia$Season == 10] <- 300
abia$Season [abia$Season == 11] <- 300

#recoding the winter months
abia$Season [abia$Season == 12] <- 400
abia$Season [abia$Season == 1] <- 400
abia$Season [abia$Season == 2] <- 400

plot_3 <- boxplot(formula = DepDelay ~ Season,
        data = abia,
        main = 'Departure delay by season',
        xlab = 'Season',
        ylab = 'Departure delay [min]',
        border = c('springgreen', 'gold', 'orange', 'skyblue'),
        names = c('Spring', 'Summer', 'Fall', 'Winter'))
```

  Looking strictly a the outlier data points, we find that there are outliers of delayed flights during the Summer and Winter Season. 

\newpage
  
## Question 3

```{r q3 load data,include=FALSE}
library(tidyverse)
library(FNN)

sclass = read.csv('sclass.csv')
sclass <- select(sclass,trim,mileage,price)

# The variables involved

# plot the data
#ggplot(data = sclass) + 
#  geom_point(mapping = aes(x = mileage, y = price), color='darkgrey')
#summary(sclass)

# Focus on 2 trim levels: 350 and 65 AMG
sclass350 = subset(sclass, trim == '350')

sclass65AMG = subset(sclass, trim == '65 AMG')

# create a function to automate everything

function_knn <- function(x,knn_size,trim){
  # Make a train-test split
  N = nrow(x)
  N_train = floor(0.8*N)
  N_test = N - N_train
  
  #####
  # Train/test split
  #####
  
  # randomly sample a set of data points to include in the training set
  train_ind = sample.int(N, N_train, replace=FALSE)
  
  # Define the training and testing set
  D_train = x[train_ind,]
  D_test = x[-train_ind,]
  
  # optional book-keeping step:
  # reorder the rows of the testing set by the mileage
  D_test = arrange(D_test, mileage)
  head(D_test)
  
  # Now separate the training and testing sets into features (X) and outcome (y)
  X_train = select(D_train, mileage)
  y_train = select(D_train, price)
  X_test = select(D_test, mileage)
  y_test = select(D_test, price)
  
  
  #####
  # Fit a few models
  #####
  
  # linear and quadratic models
  lm1 = lm(price ~ mileage, data=D_train)
  lm2 = lm(price ~ poly(mileage, 2), data=D_train)
  
  # KNN
  knn = knn.reg(train = X_train, test = X_test, y = y_train, k=knn_size)
  
  #####
  # Compare the models by RMSE_out
  #####
  
  # define a helper function for calculating RMSE
  rmse = function(y, ypred) {
    sqrt(mean(data.matrix((y-ypred)^2)))
  }
  
  ypred_lm1 = predict(lm1, X_test)
  ypred_lm2 = predict(lm2, X_test)
  ypred_knn = knn$pred
  
  rmse(y_test, ypred_lm1)
  rmse(y_test, ypred_lm2)
  rmse(y_test, ypred_knn)
  
  
  ####
  # plot the fit
  ####
  
  # attach the predictions to the test data frame
  D_test$ypred_lm2 = ypred_lm2
  D_test$ypred_knn = ypred_knn
  
  p_test = ggplot(data = D_test) + 
    geom_point(mapping = aes(x = mileage, y = price), color='black') + 
    theme_bw(base_size=18)
  p_test
  
  p_test + geom_path(aes(x = mileage, y = ypred_knn), color='blue')+ 
    geom_path(aes(x = mileage, y = ypred_lm2), color='red')+
    ggtitle(paste("S-Class",trim,"when KNN=",knn_size))
}
```

## Comparing RMSE to K
```{r load data for rmse to k,include=FALSE}
# Comparing RMSE to K for 350

rmse_k <- function(x,knn_size){

  # Make a train-test split
  N = nrow(x)
  N_train = floor(0.8*N)
  N_test = N - N_train
  
  #####
  # Train/test split
  #####
  
  # randomly sample a set of data points to include in the training set
  train_ind = sample.int(N, N_train, replace=FALSE)
  
  # Define the training and testing set
  D_train = x[train_ind,]
  D_test = x[-train_ind,]
  
  # optional book-keeping step:
  # reorder the rows of the testing set by the mileage
  D_test = arrange(D_test, mileage)
  head(D_test)
  
  # Now separate the training and testing sets into features (X) and outcome (y)
  X_train = select(D_train, mileage)
  y_train = select(D_train, price)
  X_test = select(D_test, mileage)
  y_test = select(D_test, price)
  
  
  #####
  # Fit a few models
  #####
  
  # linear and quadratic models
  lm1 = lm(price ~ mileage, data=D_train)
  lm2 = lm(price ~ poly(mileage, 2), data=D_train)
  
  # KNN
  knn = knn.reg(train = X_train, test = X_test, y = y_train, k=knn_size)
  
  #####
  # Compare the models by RMSE_out
  #####
  
  # define a helper function for calculating RMSE
  rmse = function(y, ypred) {
    sqrt(mean(data.matrix((y-ypred)^2)))
  }
  
  ypred_lm1 = predict(lm1, X_test)
  ypred_lm2 = predict(lm2, X_test)
  ypred_knn = knn$pred
  
  rmse_lm1 <<- append(rmse_lm1,rmse(y_test, ypred_lm1))
  rmse_lm2 <<- append(rmse_lm2,rmse(y_test, ypred_lm2))
  rmse_knn <<- append(rmse_knn,rmse(y_test, ypred_knn))
  knn_value <<- append(knn_value,knn_size)
  knn_max_size <<- nrow(X_train)
}

knn_value <- vector()
rmse_lm1 <- vector()
rmse_lm2 <- vector()
rmse_knn <- vector()

### Ask prof how to do this.
#for (i in 1:knn_max_size){
#  paste(rmse_k(sclass350,i))
#}

rmse_k(sclass350,10)
rmse_k(sclass350,20)
rmse_k(sclass350,30)
rmse_k(sclass350,40)
rmse_k(sclass350,50)
rmse_k(sclass350,60)
rmse_k(sclass350,70)
rmse_k(sclass350,80)
rmse_k(sclass350,90)
rmse_k(sclass350,100)
rmse_k(sclass350,110)
rmse_k(sclass350,120)
rmse_k(sclass350,130)
rmse_k(sclass350,140)
rmse_k(sclass350,150)
rmse_k(sclass350,160)
rmse_k(sclass350,170)
rmse_k(sclass350,180)
rmse_k(sclass350,190)
rmse_k(sclass350,200)
rmse_k(sclass350,210)
rmse_k(sclass350,220)
rmse_k(sclass350,230)
rmse_k(sclass350,240)
rmse_k(sclass350,250)
rmse_k(sclass350,260)
rmse_k(sclass350,270)
rmse_k(sclass350,280)
rmse_k(sclass350,290)
rmse_k(sclass350,300)
rmse_k(sclass350,310)
rmse_k(sclass350,320)
rmse_k(sclass350,330)
rmse_k(sclass350,knn_max_size)

rmse_k_df <- data.frame(knn_value,rmse_lm1,rmse_lm2,rmse_knn)

rmse_k_plot_1 <- ggplot(data=rmse_k_df)+
  geom_path(aes(x = knn_value, y = rmse_lm1), color='black')+
  geom_path(aes(x = knn_value, y = rmse_lm2), color='red')+
  geom_path(aes(x = knn_value, y = rmse_knn), color='blue')+
  xlab("KNN Value")+
  ylab("RMSE Value")+
  ggtitle("S-Class 350 RMSE to K")+
  scale_x_reverse()

# Comparing RMSE to K for 65amg
knn_value <- vector()
rmse_lm1 <- vector()
rmse_lm2 <- vector()
rmse_knn <- vector()

rmse_k(sclass65AMG,10)
rmse_k(sclass65AMG,20)
rmse_k(sclass65AMG,30)
rmse_k(sclass65AMG,40)
rmse_k(sclass65AMG,50)
rmse_k(sclass65AMG,60)
rmse_k(sclass65AMG,70)
rmse_k(sclass65AMG,80)
rmse_k(sclass65AMG,90)
rmse_k(sclass65AMG,100)
rmse_k(sclass65AMG,110)
rmse_k(sclass65AMG,120)
rmse_k(sclass65AMG,130)
rmse_k(sclass65AMG,140)
rmse_k(sclass65AMG,150)
rmse_k(sclass65AMG,160)
rmse_k(sclass65AMG,170)
rmse_k(sclass65AMG,180)
rmse_k(sclass65AMG,190)
rmse_k(sclass65AMG,200)
rmse_k(sclass65AMG,210)
rmse_k(sclass65AMG,220)
rmse_k(sclass65AMG,230)
rmse_k(sclass65AMG,knn_max_size)

rmse_k_df <- data.frame(knn_value,rmse_lm1,rmse_lm2,rmse_knn)

rmse_k_plot_2 <- ggplot(data=rmse_k_df)+
  geom_path(aes(x = knn_value, y = rmse_lm1), color='black')+
  geom_path(aes(x = knn_value, y = rmse_lm2), color='red')+
  geom_path(aes(x = knn_value, y = rmse_knn), color='blue')+
  xlab("KNN Value")+
  ylab("RMSE Value")+
  ggtitle("S-Class 65AMG RMSE to K")+
  scale_x_reverse()
```

```{r rmse_k_plot_1,echo=FALSE}
rmse_k_plot_1
```

Legend: 

* Linear RMSE in Black 

* Polynomial RMSE in Red 

* KMM RMSE in Blue 

Based on the blue line for KMM RMSE values, we find that RMSE value start to bottom out when K<150.

\newpage

```{r rmse_k_plot_2,echo=FALSE}
rmse_k_plot_2
```

Legend: 

* Linear RMSE in Black 

* Polynomial RMSE in Red 

* KMM RMSE in Blue 

Based on the blue line for KMM RMSE values, we find that RMSE value start to bottom out when K<60.

# S-Class 350

```{r q3 plot 1,echo=FALSE}
function_knn(sclass350,150,"350")
```

Legend: 

* Polynomial RMSE in Red 

* KMM RMSE in Blue 

\newpage

# S-Class 65 AMG

```{r q3 plot 2,echo=FALSE}
function_knn(sclass65AMG,60,"65 AMG")
```

Legend: 

* Polynomial RMSE in Red 

* KMM RMSE in Blue 

  In conclusion, we find that the S-Class 350 trim offers a more optimal value of K becuase of the larger dataset it provides for the training data, reducing the tail-end bias effects.