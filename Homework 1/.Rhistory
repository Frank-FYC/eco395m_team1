xlab="P-Values",
ylab="Standard Deviation",
names.arg=c("p_5_sd","p_10_sd","p_25_sd","p_50_sd","p_100_sd")
)
barplot(combined_sd, main="Sample Standard Deviations",
xlab="P-Values",
ylab="Standard Deviation",
names.arg=c("p_5_sd","p_10_sd","p_25_sd","p_50_sd","p_100_sd")
)
save.image("C:/Users/frank/Dropbox/Classes/ECO s394D Probability and Statistics [James Scott]/Homework/homework02_env.RData")
load("C:/Users/frank/Dropbox/Classes/ECO s394D Probability and Statistics [James Scott]/Homework/homework02_env.RData")
library(mosaic)
library(mosaicData)
data(TenMileRace)
help(data)
?TenMileRace
head(TenMileRace)
plot(net~age,data=TenMileRace, col='grey')
lm1 = lm(net~age,data=TenMileRace)
abline(lm1)
summary(lm1)
# Read in 10 mile race data
library(mosaic)
library(mosaicData)
data(TenMileRace)
# The model aggregrating men and women
plot(net~age,data=TenMileRace, col='grey')
lm1 = lm(net~age,data=TenMileRace)
abline(lm1)
summary(lm1)
# Now disaggregating
lmM = lm(net~age,data=subset(TenMileRace,sex=="M"))
lmF = lm(net~age,data=subset(TenMileRace,sex=="F"))
coef(lmM)
coef(lmF)
mean(net ~ sex, data=TenMileRace)
# Clearly we get different effects due to age when we disaggregate
plot(net~age,data=TenMileRace, col='grey', pch=19, cex=0.5)
abline(lm1, col='black')
abline(lmM, col='red')
abline(lmF, col='blue')
# We can model this with main effects for age and sex
lm2 = lm(net ~ age + sex, data= TenMileRace)
coef(lm2)
# A simple way to visualize the fit
plotModel(lm2)
# With an interaction
lm3 = lm(net ~ age + sex + age:sex, data= TenMileRace)
coef(lm3)
# Visualize the fit
plotModel(lm3)
# An ANOVA table
source('http://jgscott.github.io/teaching/r/utils/class_utils.R')
simple_anova(lm3)
0.05(1-0.05)/2000
library(mosaic)
library(foreach)
install.packages("ggformula")
library(mosaic)
library(ggformula)
library(ggformula)
install.packages("stringi")
library(ggformula)
library(mosaic)
library(foreach)
#####
# Example 1: inference for a binomial proportion
#####
# Goal: estimate P(cured | takes new drug)
# (as in a safety and efficacy trial, FDA phase 1)
# confidence level = 1-alpha
alpha = 0.05
# true probability
p_true = 0.25
# sample size
N = 100
# Let's simulate a single data set
results = rbinom(N, 1, p_true)
# step 1: calculate the estimator
p_hat = sum(results)/N
# step 2: plug in estimates of unknown quantities into
# analytical formula for standard error
se_hat = sqrt(p_hat * (1-p_hat)/N)
# step 3: form confidence interval based on asymptotic normality assumption
z_crit = qnorm(1 - alpha/2)
lower_bound = p_hat - z_crit*se_hat
upper_bound = p_hat + z_crit*se_hat
lower_bound
upper_bound
# check whether the interval contained the true value
(p_true > lower_bound) & (p_true < upper_bound)
# compare the normal CI with the bootstrapped CI
boot1 = do(1000)*{
results_bootstrap = resample(results)
p_hat_boot = sum(results_bootstrap)/N
}
# really close
confint(boot1)
# Let's repeat this in a Monte Carlo simulation
NMC = 1000
sim1 = do(NMC)*{
results = rbinom(N, 1, p_true)
p_hat = sum(results)/N
se_hat = sqrt(p_hat * (1-p_hat)/N)
lower_bound = p_hat - z_crit*se_hat
upper_bound = p_hat + z_crit*se_hat
(p_true > lower_bound) & (p_true < upper_bound)
}
sum(sim1$result)/NMC
# now repeat for many different values of N
n_grid = seq(6, 100, by=2)
clt_sim1 = foreach(N = n_grid, .combine='c') %do% { # %do% has an error
this_sim = do(NMC)*{
results = rbinom(N, 1, p_true)
p_hat = sum(results)/N
se_hat = sqrt(p_hat * (1-p_hat)/N)
lower_bound = p_hat - z_crit*se_hat
upper_bound = p_hat + z_crit*se_hat
(p_true > lower_bound) & (p_true < upper_bound)
}
sum(this_sim$result)/NMC
}
plot(n_grid, clt_sim1)
this_sim = do(NMC)*{
y = sample(population,N)
mu_hat = mean(y)
se_hat = sd(y)/sqrt(N)
lower_bound = mu_hat - z_crit*se_hat
upper_bound = muhat + z_crit*se_hat
(mu_true > lower_bound) & (mu_true < upper_bound)
}
sum(this_sim$result)/NMC
population
library(readr)
predimed <- read_csv("C:/Users/frank/Dropbox/Classes/ECO s394D Probability and Statistics [James Scott]/R Classes/2018-08-15 CI Simulation/predimed.csv")
xtabs(~event + group, data=predimed)
N = (97+1945)
p_phat = 97/N
p_phat
p_hat = 97/N
p_hat
N = (97+1945)
p_hat = 97/N
# step 2: std error
se_hat = sqrt(p_hat * (1-p_hat)/N)
se_hat
se_hat = sqrt(p_hat * (1-p_hat)/N)
z_crit = qnorm(1-0.05/2)
p_hat - z_crit*se_hat
p_hat + z_crit*se_hat
z_crit = qnorm((1-0.05)/2)
z_crit
z_crit = qnorm(1-0.05/2)
z_crit
citation()
demo()
quit()
update
help update
install.packages(c("backports", "BH", "broom", "cli", "colorspace", "digest", "dplyr", "evaluate", "fansi", "ggformula", "ggplot2", "jsonlite", "knitr", "markdown", "mime", "mosaic", "packrat", "pillar", "pkgconfig", "R6", "Rcpp", "readr", "RJSONIO", "rlang", "rmarkdown", "rsconnect", "rstudioapi", "stringi", "tibble", "tidyr", "tidyselect", "tinytex", "xfun"))
install.packages(c("backports", "BH", "broom", "cli", "colorspace", "digest", "dplyr", "evaluate", "fansi", "ggformula", "ggplot2", "jsonlite", "knitr", "markdown", "mime", "mosaic", "packrat", "pillar", "pkgconfig", "R6", "Rcpp", "readr", "RJSONIO", "rlang", "rmarkdown", "rsconnect", "rstudioapi", "stringi", "tibble", "tidyr", "tidyselect", "tinytex", "xfun"))
install.packages(c("backports", "BH", "broom", "cli", "colorspace", "digest", "dplyr", "evaluate", "fansi", "ggformula", "ggplot2", "jsonlite", "knitr", "markdown", "mime", "mosaic", "packrat", "pillar", "pkgconfig", "R6", "Rcpp", "readr", "RJSONIO", "rlang", "rmarkdown", "rsconnect", "rstudioapi", "stringi", "tibble", "tidyr", "tidyselect", "tinytex", "xfun"))
install.packages("rmarkdown")
# 1100: Why plots? So we can co-opt our visual processing to better understand numerical ratios and effectively judge magnitude (no truncating axis).
# 1100: Why plots? So we can co-opt our visual processing to better understand numerical ratios and effectively judge magnitude (no truncating axis).
library(tidyverse)
install.packages("purr")
install.packages("purrr")
library(tidyverse)
data(mpg)
mpg
# creating a ggplot
# The first line sets up a coordinate system.
# the second line maps displ to x, hwy to y, and draws points
ggplot(data = mpg) +
geom_point(mapping = aes(x = displ, y = hwy))
# aesthetic mappings can get more complicated:
# here we vary point color by some third variable
ggplot(data = mpg) +
geom_point(mapping = aes(x = displ, y = hwy, color = class))
# Lots of options for point characeristics that can be changed.
# Some aesthetic mappings are more effective than others!
# For example, compare the following:
# size of point
ggplot(data = mpg) +
geom_point(mapping = aes(x = displ, y = hwy, size = class))
# transparency
ggplot(data = mpg) +
geom_point(mapping = aes(x = displ, y = hwy, alpha = class))
# point shape
ggplot(data = mpg) +
geom_point(mapping = aes(x = displ, y = hwy, shape = class))
# adding a title
ggplot(mpg, aes(displ, hwy)) +
geom_point(aes(color = class)) +
labs(title = "Fuel efficiency generally decreases with engine size")
# manually setting an aesthetic property
ggplot(data = mpg) +
geom_point(mapping = aes(x = displ, y = hwy), color = "blue")
# note: compare with
ggplot(data = mpg) +
geom_point(mapping = aes(x = displ, y = hwy, color = "blue"))
# faceting on two variables
ggplot(data = mpg) +
geom_point(mapping = aes(x = displ, y = hwy)) +
facet_grid(drv ~ cyl)
# axis labels
ggplot(mpg, aes(displ, hwy)) +
geom_point(aes(colour = class)) +
geom_smooth(se = FALSE) +
labs(
x = "Engine displacement (L)",
y = "Highway fuel economy (mpg)",
colour = "Car type"
)
# axis labels
ggplot(mpg, aes(displ, hwy)) +
geom_point(aes(colour = class)) +
geom_smooth(se = FALSE) +
labs(
x = "Engine displacement (L)",
y = "Highway fuel economy (mpg)",
colour = "Car type"
)
library(tidyverse)
TitanicSurvival = read.csv('../data/TitanicSurvival.csv')
summary(TitanicSurvival)
# a bit un-interesting
ggplot(data = TitanicSurvival) +
geom_bar(mapping = aes(x = survived))
# Let's create a new data set of survival percentages
# We'll group by sex
d1 = TitanicSurvival %>%
group_by(sex) %>%
summarize(surv_pct = sum(survived=='yes')/n())
d1
TitanicSurvival = read.csv('./data/TitanicSurvival.csv')
summary(TitanicSurvival)
TitanicSurvival = read.csv('../data/TitanicSurvival.csv')
summary(TitanicSurvival)
library(tidyverse)
TitanicSurvival = read.csv('../data/TitanicSurvival.csv')
summary(TitanicSurvival)
TitanicSurvival = read.csv('.../data/TitanicSurvival.csv')
library(tidyverse)
TitanicSurvival = read.csv('../data/TitanicSurvival.csv')
TitanicSurvival = read.csv('..\data\TitanicSurvival.csv')
TitanicSurvival = read.csv('../data/TitanicSurvival.csv')
cd
wd
sd
cd
getwd
getwd()
this.dir <- dirname(parent.frame(2)$ofile)
setwd(this.dir)
install.packages("FNN")
library(tidyverse)
library(FNN)
# read in the data: make sure to use the path name to
# wherever you'd stored the file
loadhou = read.csv('C:\Dropbox\Classes\ECO 395M Data mining and statisical learning\github_clone\ECO395M\data')
library(tidyverse)
library(FNN)
# read in the data: make sure to use the path name to
# wherever you'd stored the file
loadhou = read.csv('C:/Dropbox/Classes/ECO 395M Data mining and statisical learning/github_clone/ECO395M/data')
loadhou = read.csv('C:/Dropbox/Classes/ECO 395M Data mining and statisical learning/github_clone/ECO395M/data/loadhou.csv')
summary(loadhou)
# plot the data
ggplot(data = loadhou) +
geom_point(mapping = aes(x = KHOU, y = COAST), color='darkgrey') +
ylim(7000, 20000)
# Make a train-test split
N = nrow(loadhou)
N_train = floor(0.8*N)
N_test = N - N_train
loadhou = read.csv('./github_clone/ECO395M/data/loadhou.csv')
loadhou = read.csv('../github_clone/ECO395M/data/loadhou.csv')
loadhou = read.csv('C:/Dropbox/Classes/ECO 395M Data mining and statisical learning/github_clone/ECO395M/data/loadhou.csv')
library(tidyverse)
library(FNN)
# read in the data: make sure to use the path name to
# wherever you'd stored the file
loadhou = read.csv('C:/Dropbox/Classes/ECO 395M Data mining and statisical learning/github_clone/ECO395M/data/loadhou.csv')
summary(loadhou)
# plot the data
ggplot(data = loadhou) +
geom_point(mapping = aes(x = KHOU, y = COAST), color='darkgrey') +
ylim(7000, 20000)
# Make a train-test split
N = nrow(loadhou)
N_train = floor(0.8*N)
N_test = N - N_train
#####
# Train/test split
#####
# randomly sample a set of data points to include in the training set
train_ind = sample.int(N, N_train, replace=FALSE)
# Define the training and testing set
D_train = loadhou[train_ind,]
D_test = loadhou[-train_ind,]
gwd
wd
getwd
getwd()
library(tidyverse)
library(FNN)
# read in the data: make sure to use the path name to
# wherever you'd stored the file
loadhou = read.csv('C:/Dropbox/Classes/ECO 395M Data mining and statisical learning/github_clone/ECO395M/data/loadhou.csv')
summary(loadhou)
# plot the data
ggplot(data = loadhou) +
geom_point(mapping = aes(x = KHOU, y = COAST), color='darkgrey') +
ylim(7000, 20000)
# Make a train-test split
N = nrow(loadhou)
N_train = floor(0.8*N)
N_test = N - N_train
#####
# Train/test split
#####
# randomly sample a set of data points to include in the training set
train_ind = sample.int(N, N_train, replace=FALSE)
# Define the training and testing set
D_train = loadhou[train_ind,]
D_test = loadhou[-train_ind,]
# optional book-keeping step:
# reorder the rows of the testing set by the KHOU (temperature) variable
# this isn't necessary, but it will allow us to make a pretty plot later
D_test = arrange(D_test, KHOU)
head(D_test)
# Now separate the training and testing sets into features (X) and outcome (y)
X_train = select(D_train, KHOU)
y_train = select(D_train, COAST)
X_test = select(D_test, KHOU)
y_test = select(D_test, COAST)
#####
# Fit a few models
#####
# linear and quadratic models
lm1 = lm(COAST ~ KHOU, data=D_train)
lm2 = lm(COAST ~ poly(KHOU, 2), data=D_train)
# KNN 250
knn250 = knn.reg(train = X_train, test = X_test, y = y_train, k=250)
names(knn250)
library(tidyverse)
library(FNN)
# read in the data: make sure to use the path name to
# wherever you'd stored the file
loadhou = read.csv('C:/Dropbox/Classes/ECO 395M Data mining and statisical learning/github_clone/ECO395M/data/loadhou.csv')
summary(loadhou)
# plot the data
ggplot(data = loadhou) +
geom_point(mapping = aes(x = KHOU, y = COAST), color='darkgrey') +
ylim(7000, 20000)
# Make a train-test split
N = nrow(loadhou)
N_train = floor(0.8*N)
N_test = N - N_train
#####
# Train/test split
#####
# randomly sample a set of data points to include in the training set
train_ind = sample.int(N, N_train, replace=FALSE)
# Define the training and testing set
D_train = loadhou[train_ind,]
D_test = loadhou[-train_ind,]
# optional book-keeping step:
# reorder the rows of the testing set by the KHOU (temperature) variable
# this isn't necessary, but it will allow us to make a pretty plot later
D_test = arrange(D_test, KHOU)
head(D_test)
# Now separate the training and testing sets into features (X) and outcome (y)
X_train = select(D_train, KHOU)
y_train = select(D_train, COAST)
X_test = select(D_test, KHOU)
y_test = select(D_test, COAST)
#####
# Fit a few models
#####
# linear and quadratic models
lm1 = lm(COAST ~ KHOU, data=D_train)
lm2 = lm(COAST ~ poly(KHOU, 2), data=D_train)
# KNN 250
knn250 = knn.reg(train = X_train, test = X_test, y = y_train, k=250)
names(knn250)
#####
# Compare the models by RMSE_out
#####
# define a helper function for calculating RMSE
rmse = function(y, ypred) {
sqrt(mean((y-ypred)^2))
}
ypred_lm1 = predict(lm1, X_test)
ypred_lm2 = predict(lm2, X_test)
ypred_knn250 = knn250$pred
rmse(y_test, ypred_lm1)
rmse(y_test, ypred_lm2)
rmse(y_test, ypred_knn250)
ypred_lm1 = predict(lm1, X_test)
ypred_lm2 = predict(lm2, X_test)
ypred_knn250 = knn250$pred
rmse(y_test, ypred_lm1)
rmse(y_test, ypred_lm2)
rmse(y_test, ypred_knn250)
ypred_lm1
y_test
ypred_lm1
rmse(y_test, ypred_lm1)
rmse(y_test, ypred_lm2)
rmse(y_test, ypred_knn250)
rmse = function(y, x) {
sqrt(mean((y-ypred)^2))
}
rmse(y_test, ypred_lm1)
rmse(y_test, ypred_lm2)
rmse(y_test, ypred_knn250)
ypred_lm1 = predict(lm1, X_test)
ypred_lm2 = predict(lm2, X_test)
ypred_knn250 = knn250$pred
rmse(y_test, ypred_lm1)
rmse(y_test, ypred_lm2)
rmse(y_test, ypred_knn250)
rmse = function(y, x) {
sqrt(mean((y-x)^2))
}
ypred_lm1 = predict(lm1, X_test)
ypred_lm2 = predict(lm2, X_test)
ypred_knn250 = knn250$pred
rmse(y_test, ypred_lm1)
rmse(y_test, ypred_lm2)
rmse(y_test, ypred_knn250)
y_test
ypred_lm1
rmse = function(y, ypred) {
sqrt(mean((y-as.numeric(ypred))^2))
}
ypred_lm1 = predict(lm1, X_test)
ypred_lm2 = predict(lm2, X_test)
ypred_knn250 = knn250$pred
rmse(y_test, ypred_lm1)
rmse(y_test, ypred_lm2)
rmse(y_test, ypred_knn250)
as.numeric(ypred_lm1)
rmse(y_test, ypred_lm1)
y_test
as.numeric(ypred_lm1)
data.matrix(ypred_lm1)
rmse(y_test, ypred_lm1)
rmse = function(y, ypred) {
sqrt(mean((data.matrix(y-ypred))^2))
}
ypred_lm1 = predict(lm1, X_test)
ypred_lm2 = predict(lm2, X_test)
ypred_knn250 = knn250$pred
rmse(y_test, ypred_lm1)
rmse(y_test, ypred_lm2)
rmse(y_test, ypred_knn250)
# attach the predictions to the test data frame
D_test$ypred_lm2 = ypred_lm2
D_test$ypred_knn250 = ypred_knn250
p_test = ggplot(data = D_test) +
geom_point(mapping = aes(x = KHOU, y = COAST), color='lightgrey') +
theme_bw(base_size=18) +
ylim(7000, 20000)
p_test
p_test + geom_point(aes(x = KHOU, y = ypred_knn250), color='red')
p_test + geom_path(aes(x = KHOU, y = ypred_knn250), color='red')
p_test + geom_path(aes(x = KHOU, y = ypred_knn250), color='red') +
geom_path(aes(x = KHOU, y = ypred_lm2), color='blue')
N
source('C:/Dropbox/Classes/ECO 395M Data mining and statisical learning/2019-02-06 Class/in-class exercise.R', echo=TRUE)
n
N
source('C:/Dropbox/Classes/ECO 395M Data mining and statisical learning/2019-02-06 Class/in-class exercise.R', echo=TRUE)
N
loadhou = loadhou[sample(nrow(loadhou), 150), ]
loadhou
source('C:/Dropbox/Classes/ECO 395M Data mining and statisical learning/2019-02-06 Class/in-class exercise.R', echo=TRUE)
source('C:/Dropbox/Classes/ECO 395M Data mining and statisical learning/2019-02-06 Class/in-class exercise.R', echo=TRUE)
source('C:/Dropbox/Classes/ECO 395M Data mining and statisical learning/2019-02-06 Class/in-class exercise.R', echo=TRUE)
source('C:/Dropbox/Classes/ECO 395M Data mining and statisical learning/2019-02-06 Class/in-class exercise.R', echo=TRUE)
source('C:/Dropbox/Classes/ECO 395M Data mining and statisical learning/2019-02-06 Class/in-class exercise.R', echo=TRUE)
source('C:/Dropbox/Classes/ECO 395M Data mining and statisical learning/2019-02-06 Class/in-class exercise.R', echo=TRUE)
source('C:/Dropbox/Classes/ECO 395M Data mining and statisical learning/2019-02-06 Class/in-class exercise.R', echo=TRUE)
source('C:/Dropbox/Classes/ECO 395M Data mining and statisical learning/2019-02-06 Class/in-class exercise.R', echo=TRUE)
source('C:/Dropbox/Classes/ECO 395M Data mining and statisical learning/2019-02-06 Class/in-class exercise.R', echo=TRUE)
loadhou = loadhou[sample.int(nrow(loadhou), 150), replace=FALSE]
source('C:/Dropbox/Classes/ECO 395M Data mining and statisical learning/2019-02-06 Class/in-class exercise.R', echo=TRUE)
source('C:/Dropbox/Classes/ECO 395M Data mining and statisical learning/2019-02-06 Class/in-class exercise.R', echo=TRUE)
source('C:/Dropbox/Classes/ECO 395M Data mining and statisical learning/2019-02-06 Class/in-class exercise.R', echo=TRUE)
source('C:/Dropbox/Classes/ECO 395M Data mining and statisical learning/2019-02-06 Class/in-class exercise.R', echo=TRUE)
loadhou_sample = loadhou[sample.int(nrow(loadhou), 150), ]
loadhou_sample
source('C:/Dropbox/Classes/ECO 395M Data mining and statisical learning/2019-02-06 Class/in-class exercise.R', echo=TRUE)
source('C:/Dropbox/Classes/ECO 395M Data mining and statisical learning/2019-02-06 Class/in-class exercise.R', echo=TRUE)
source('C:/Dropbox/Classes/ECO 395M Data mining and statisical learning/2019-02-06 Class/in-class exercise.R', echo=TRUE)
source('C:/Dropbox/Classes/ECO 395M Data mining and statisical learning/2019-02-06 Class/in-class exercise.R', echo=TRUE)
greenbuildings <- read.csv("C:/Dropbox/Classes/ECO 395M Data mining and statisical learning/github_clone/eco395m_team1/Homework 1/greenbuildings.csv")
View(greenbuildings)
greenbuildings <- read.csv("./greenbuildings.csv")
greenbuildings <- read.csv("/greenbuildings.csv")
greenbuildings <- read.csv('/greenbuildings.csv')
greenbuildings <- read.csv('eco395m_team1/greenbuildings.csv')
greenbuildings <- read.csv('./eco395m_team1/greenbuildings.csv')
greenbuildings <- read.csv('/eco395m_team1/greenbuildings.csv')
gwd
getwd
source('C:/Dropbox/Classes/ECO 395M Data mining and statisical learning/github_clone/eco395m_team1/Homework 1/script.R', echo=TRUE)
greenbuildings <- read.csv('greenbuildings.csv')
