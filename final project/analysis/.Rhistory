install.packages("tm")
install.packages("slam")
install.packages("proxy")
## The tm library and related plugins comprise R's most popular text-mining stack.
## See http://cran.r-project.org/web/packages/tm/vignettes/tm.pdf
library(tm)
library(magrittr)
library(slam)
library(proxy)
setwd("C:/Dropbox/Classes/ECO 395M Data mining and statisical learning/2019-05-08 Week 15 Trees continued, text")
## Test it on Adam Smith
adam = readerPlain("../data/division_of_labor.txt")
## Test it on Adam Smith
adam = readerPlain("../ECO395M/data/division_of_labor.txt")
## Test it on Adam Smith
adam = readerPlain(".../ECO395M/data/division_of_labor.txt")
## Test it on Adam Smith
adam = readerPlain("./ECO395M/data/division_of_labor.txt")
## Test it on Adam Smith
adam = readerPlain('./ECO395M/data/division_of_labor.txt')
## Test it on Adam Smith
adam = readerPlain('../ECO395M/data/division_of_labor.txt')
## Test it on Adam Smith
adam = readerPlain('.../ECO395M/data/division_of_labor.txt')
## Test it on Adam Smith
adam = readerPlain('..../ECO395M/data/division_of_labor.txt')
## Test it on Adam Smith
adam = readerPlain('../ECO395M/data/division_of_labor.txt')
## The tm library and related plugins comprise R's most popular text-mining stack.
## See http://cran.r-project.org/web/packages/tm/vignettes/tm.pdf
library(tm)
library(magrittr)
library(slam)
library(proxy)
## tm has many "reader" functions.  Each one has
## arguments elem, language, id
## (see ?readPlain, ?readPDF, ?readXML, etc)
## This wraps another function around readPlain to read
## plain text documents in English.
# I've stored this function as a Github "gist" at:
# https://gist.github.com/jgscott/28d9d1287a0c3c1477e2113f6758d5ff
readerPlain = function(fname){
readPlain(elem=list(content=readLines(fname)),
id=fname, language='en') }
## Test it on Adam Smith
adam = readerPlain('../ECO395M/data/division_of_labor.txt')
## apply to all of Simon Cowell's articles
## (probably not THE Simon Cowell: https://twitter.com/simoncowell)
## "globbing" = expanding wild cards in filename paths
file_list = Sys.glob('../ECO395Mdata/ReutersC50/C50train/SimonCowell/*.txt')
source('C:/Dropbox/Classes/ECO 395M Data mining and statisical learning/2019-05-08 Week 15 Trees continued, text/tm_examples.R', echo=TRUE)
## You can inspect its entries...
inspect(DTM_simon[1:10,1:20])
## ...find words with greater than a min count...
findFreqTerms(DTM_simon, 50)
## ...or find words whose count correlates with a specified word.
findAssocs(DTM_simon, "genetic", .5)
## Finally, drop those terms that only occur in one or two documents
## This is a common step: the noise of the "long tail" (rare terms)
##	can be huge, and there is nothing to learn if a term occured once.
## Below removes those terms that have count 0 in >95% of docs.
## Probably a bit stringent here... but only 50 docs!
DTM_simon = removeSparseTerms(DTM_simon, 0.95)
DTM_simon # now ~ 1000 terms (versus ~3000 before)
# construct TF IDF weights
tfidf_simon = weightTfIdf(DTM_simon)
inspect(tfidf_simon[1,])
inspect(tfidf_simon[2,])
inspect(tfidf_simon[3,])
# could go back to the raw corpus
content(simon[[1]])
content(simon[[2]])
content(simon[[3]])
# cosine similarity
i = 1
j = 3
sum(tfidf_simon[i,] * (tfidf_simon[j,]))/(sqrt(sum(tfidf_simon[i,]^2)) * sqrt(sum(tfidf_simon[j,]^2)))
# the full set of cosine similarities
# two helper functions that use some linear algebra for the calculations
cosine_sim_docs = function(dtm) {
crossprod_simple_triplet_matrix(t(dtm))/(sqrt(col_sums(t(dtm)^2) %*% t(col_sums(t(dtm)^2))))
}
# use the function to compute pairwise cosine similarity for all documents
cosine_sim_mat = cosine_sim_docs(tfidf_simon)
# Now consider a query document
content(simon[[17]])
cosine_sim_mat[17,]
# looks like document 16 has the highest cosine similarity
sort(cosine_sim_mat[17,], decreasing=TRUE)
content(simon[[16]])
content(simon[[17]])
# define the cosine distance
cosine_dist_mat = proxy::dist(as.matrix(tfidf_simon), method='cosine')
tree_simon = hclust(cosine_dist_mat)
plot(tree_simon)
clust5 = cutree(tree_simon, k=5)
# inspect the clusters
which(clust5 == 1)
content(simon[[1]])
content(simon[[4]])
content(simon[[5]])
# Now PCA on term frequencies
X = as.matrix(tfidf_simon)
summary(colSums(X))
scrub_cols = which(colSums(X) == 0)
X = X[,-scrub_cols]
pca_simon = prcomp(X, scale=TRUE)
plot(pca_simon)
# Look at the loadings
pca_simon$rotation[order(abs(pca_simon$rotation[,1]),decreasing=TRUE),1][1:25]
## Look at the first two PCs..
# We've now turned each document into a single pair of numbers -- massive dimensionality reduction
pca_simon$x[,1:2]
pca_simon$rotation[order(abs(pca_simon$rotation[,2]),decreasing=TRUE),2][1:25]
plot(pca_simon$x[,1:2], xlab="PCA 1 direction", ylab="PCA 2 direction", bty="n",
type='n')
text(pca_simon$x[,1:2], labels = 1:length(simon), cex=0.7)
# Both about "Scottish Amicable"
content(simon[[46]])
content(simon[[48]])
# Both about genetic testing
content(simon[[25]])
content(simon[[26]])
# Both about Ladbroke's merger
content(simon[[10]])
content(simon[[11]])
# Now look at the word view
# 5-dimensional word vectors
word_vectors = pca_simon$rotation[,1:5]
word_vectors[984,]
d_mat = dist(t(word_vectors))
## Test it on Adam Smith
adam = readerPlain('../ECO395M/data/division_of_labor.txt')
adam
meta(adam)
content(adam)
library(tidyverse)
library(foreign)
df <- read.dta("../data/nlsy_deming.dta")
df2 <- read.dta("../data/data_Deming_2008_0217.dta")
df_headstart <- subset(df,head_start==1)
df_nonheadstart <- subset(df,head_start==0)
sum()
setwd("C:/Dropbox/Classes/ECO 395M Data mining and statisical learning/eco395m_team_awesome/final project/analysis")
source('C:/Dropbox/Classes/ECO 395M Data mining and statisical learning/eco395m_team_awesome/final project/analysis/frank_script.R', echo=TRUE)
source('C:/Dropbox/Classes/ECO 395M Data mining and statisical learning/eco395m_team_awesome/final project/analysis/frank_script.R', echo=TRUE)
names(df2)
birthweight <- subse(df2,BirthWeight==1)
birthweight <- subset(df2,BirthWeight==1)
View(birthweight)
birthweight <- subset(df2,BirthWeight==0:100)
birthweight <- subset(df2,BirthWeight=0:100)
View(birthweight)
